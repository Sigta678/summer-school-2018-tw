{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n"
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "- ### Ex.1-1 Create placeholder\n",
    "- ### Ex.1-2 Define the weights\n",
    "- ### Ex.1-3 Understand the layers\n",
    "- ### Ex.2-1 Choose the cost function\n",
    "- ### Ex.2-2 Tune the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex.1-1 Create placeholder: \n",
    "\n",
    "> ## Discussion:\n",
    ">> ## 1. What's the meaning of <span style=\"color:blue\">None</span> in tf.placeholder( XX ,[<span style=\"color:blue\">None</span>, XX]) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_H0, n_W0, n_C0, n_y, n_prop1, n_prop2):\n",
    "    # Create float placeholder for input_picture\n",
    "    pic_placehold = tf.placeholder(tf.float32,[None,n_H0,n_W0,n_C0])\n",
    "    # Create integer placeholder for output_class\n",
    "    action_placehold = tf.placeholder(tf.int32,[None,n_y])\n",
    "    \n",
    "    ### START CODE HERE ### (1 line)\n",
    "    property_placehold = tf.placeholder(tf.int32, [None, n_prop1, n_prop2])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return pic_placehold, action_placehold, property_placehold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune your placeholder dimensions\n",
    "> ## input_pic: 256\\*256*3\n",
    "> ## output_action: 6\n",
    "> ## input_property: 4\\*15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tune the placeholder dimensions if you want\n",
    "pic_height = 256\n",
    "pic_width = 256\n",
    "pic_channel = 3\n",
    "\n",
    "output_class = 6\n",
    "\n",
    "input_prop1 = 4\n",
    "input_prop2 = 15\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pic_placehold = Tensor(\"Placeholder_3:0\", shape=(?, 256, 256, 3), dtype=float32)\n",
      "action_placehold = Tensor(\"Placeholder_4:0\", shape=(?, 6), dtype=int32)\n",
      "state_placehold = Tensor(\"Placeholder_5:0\", shape=(?, 4, 15), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "pic_placehold, action_placehold, state_placehold = create_placeholders(pic_height, pic_width, pic_channel, \n",
    "                                                                       output_class, input_prop1, input_prop2)\n",
    "print (\"pic_placehold = \" + str(pic_placehold))\n",
    "print (\"action_placehold = \" + str(action_placehold))\n",
    "print (\"state_placehold = \" + str(state_placehold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "<table> \n",
    "<tr>\n",
    "<td>\n",
    "    pic_placehold = Tensor(\"Placeholder:0\", shape=(?, 256, 256, 3), dtype=float32)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "    action_placehold  = Tensor(\"Placeholder_1:0\", shape=(?, 6), dtype=int32)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>\n",
    "    state_placehold  = Tensor(\"Placeholder_2:0\", shape=(?, 4, 15), dtype=int32)\n",
    "        \n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex.1-2 Define the weights: \n",
    "> ## Discussion:\n",
    ">> ## 1. What's the meaning of channel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(filter_height, filter_width, channel_in, channel_out):\n",
    "    \n",
    "    w_parameters = tf.Variable(tf.random_normal([filter_height,filter_width,channel_in,channel_out], stddev=0.1))\n",
    "    b_parameters = tf.Variable(tf.constant(0.1, shape=[channel_out]))\n",
    "    \n",
    "    return w_parameters, b_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune your filter dimensions\n",
    "> ## filter size: 5\\*5\n",
    "> ## how many filters you use in the previous layer: 3\n",
    "> ## total number of filters: 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tune the filters if you want\n",
    "filter_height = 5\n",
    "filter_width = 5\n",
    "channel_in = 3\n",
    "channel_out = 8\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your filter (at channel_in: 0 channel_out: 0) looks like this\n",
      "\n",
      "0.12432758\t-0.19361599\t0.2630479\t-0.023713544\t-0.117190026\t\n",
      "-0.032849725\t0.12854983\t-0.07764306\t0.13949698\t-0.146802\t\n",
      "-0.119006775\t0.03017266\t0.023374086\t0.12396772\t0.1453821\t\n",
      "-0.061110675\t-0.012775389\t0.121099785\t0.068862036\t0.05097431\t\n",
      "0.054174285\t0.055661645\t-0.081649646\t-0.096412376\t-0.09381925\t\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess_test:\n",
    "    w,b = initialize_parameters(filter_height, filter_width, channel_in, channel_out)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess_test.run(init)\n",
    "    # for 4D arrays, back dimensions to the front\n",
    "\n",
    "    ### Watch for specific channel\n",
    "    c_in = 0 # 0 ~ channel_in-1\n",
    "    c_out = 0 # 0 ~ channel_out-1\n",
    "    ###\n",
    "    print(\"Your filter (at channel_in: %d channel_out: %d) looks like this\\n\" %(c_in,c_out) )\n",
    "    for i in range(0,int(w.shape[0])):\n",
    "        for j in range(0,int(w.shape[1])):\n",
    "            #print(w[i,j].shape)\n",
    "            print(w[i,j].eval()[c_in,c_out], end='\\t')\n",
    "        print()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex.1-3 Understand the layers\n",
    "> ## Discussion:\n",
    ">> ## 1. What is stride? How does it correlate to filter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholder\n",
    "pic_height = 256\n",
    "pic_width = 256\n",
    "pic_channel = 3\n",
    "pic_placehold = tf.placeholder(tf.float32,[None,pic_height,pic_width,pic_channel])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(tensor_in, channel_in, channel_out, filter_height, filter_width, strides_height, strides_width,\n",
    "               maxPool_height=2, maxPool_width=2):\n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    # Define the weights\n",
    "    w = tf.Variable(tf.truncated_normal([filter_height,filter_width,channel_in,channel_out], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[channel_out]))\n",
    "    \n",
    "    # Understand the layer\n",
    "    conv = tf.nn.conv2d(tensor_in, w, strides=[1, strides_height, strides_width,1], padding='SAME')                \n",
    "    print(\"Conv\\t\\t%d\\t%d\\t%d\\t\\t| %s\" % (conv.shape[1],conv.shape[2],conv.shape[3],conv.shape))\n",
    "    # Activate each nodes with bias\n",
    "    activate_func = tf.nn.relu(conv + b)\n",
    "    print(\"ReLU\\t\\t%d\\t%d\\t%d\\t\\t| %s\" % (activate_func.shape[1],activate_func.shape[2],activate_func.shape[3],activate_func.shape))\n",
    "    \n",
    "    maxPool = tf.nn.max_pool(activate_func, ksize=[1,maxPool_height,maxPool_width,1], strides=[1,maxPool_height,maxPool_width,1], padding='SAME')\n",
    "    print(\"max_pool\\t%d\\t%d\\t%d\\t\\t| %s\" %(maxPool.shape[1],maxPool.shape[2],maxPool.shape[3],maxPool.shape))\n",
    "    return maxPool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN layer and Max pooling layer -- Tune the strides (Notice that max pooling size is fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "Layer\t\tHeight\tWidth\tChannel\tNode\t| What you will see\n",
      "---------------------------------------------------------------\n",
      "Input\t\t256\t256\t3\t\t| (?, 256, 256, 3)\n",
      "Conv\t\t256\t256\t8\t\t| (?, 256, 256, 8)\n",
      "ReLU\t\t256\t256\t8\t\t| (?, 256, 256, 8)\n",
      "max_pool\t128\t128\t8\t\t| (?, 128, 128, 8)\n"
     ]
    }
   ],
   "source": [
    "s = \"---------------------------------------------------------------\"\n",
    "print(s)\n",
    "print(\"Layer\\t\\tHeight\\tWidth\\tChannel\\tNode\\t| What you will see\")\n",
    "print(s)\n",
    "print(\"Input\\t\\t%d\\t%d\\t%d\\t\\t| %s\" %(pic_placehold.shape[1],pic_placehold.shape[2],pic_placehold.shape[3],pic_placehold.shape))\n",
    "\n",
    "### Tunning part:\n",
    "strides_height = 1\n",
    "strides_width = 1\n",
    "filter_height = 4\n",
    "filter_width = 4\n",
    "###\n",
    "\n",
    "conv1 = conv_block(pic_placehold, 3, 8, filter_height, filter_width, strides_height, strides_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 1: Expected Output **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Input | (?, 256, 256, 3)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Conv | (?, 128, 128, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ReLU | (?, 128, 128, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            max_pool | (?, 64, 64, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the channel in, tune the channel out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv\t\t32\t64\t10\t\t| (?, 32, 64, 10)\n",
      "ReLU\t\t32\t64\t10\t\t| (?, 32, 64, 10)\n",
      "max_pool\t16\t32\t10\t\t| (?, 16, 32, 10)\n"
     ]
    }
   ],
   "source": [
    "### Tunning part:\n",
    "channel_in = int(conv1.shape[3])\n",
    "channel_out = 10\n",
    "###\n",
    "strides_height = 4\n",
    "strides_width = 2\n",
    "filter_height = 4\n",
    "filter_width = 4\n",
    "\n",
    "conv2 = conv_block(conv1, channel_in, channel_out, filter_height, filter_width, strides_height, strides_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 2: Expected Output **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Conv | (?, 16, 32, 16)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ReLU | (?, 16, 32, 16)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            max_pool | (?, 8, 16, 16)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune the strides again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv\t\t16\t32\t32\t\t| (?, 16, 32, 32)\n",
      "ReLU\t\t16\t32\t32\t\t| (?, 16, 32, 32)\n",
      "max_pool\t16\t32\t32\t\t| (?, 16, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "### Tunning part:\n",
    "strides_height = 1\n",
    "strides_width = 1\n",
    "max_pool_height = 1 \n",
    "max_pool_width = 1\n",
    "###\n",
    "channel_in = int(conv2.shape[3])\n",
    "channel_out = 32\n",
    "filter_height = 4\n",
    "filter_width = 4\n",
    "\n",
    "conv3 = conv_block(conv2, channel_in, channel_out, filter_height, filter_width, strides_height, strides_width\n",
    "                  , max_pool_height, max_pool_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 3: Expected Output **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Conv | (?, 4, 6, 32)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ReLU | (?, 4, 6, 32)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            max_pool | (?, 2, 2, 32)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex.1-4 Change the nodes\n",
    "> ## Discussion:\n",
    ">> ## 1. Why do we change the nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_block(tensor_in, node_out, activate_func):\n",
    "    node_in = int(tensor_in.shape[1])\n",
    "    w = tf.Variable(tf.random_normal([node_in, node_out], stddev=0.35))\n",
    "    b = tf.Variable(tf.zeros([node_out]))\n",
    "    \n",
    "    if activate_func == None:\n",
    "        ### 2 way\n",
    "        #fully_connect = tf.contrib.layers.fully_connected(tensor_in, num_outputs= node_out,activation_fn= None)\n",
    "        fully_connect = tf.add(tf.matmul(tensor_in,w), b)\n",
    "        ###\n",
    "        print(\"fully_connect\\t\\t\\t\\t%d\\t| %s\" % (fully_connect.shape[1],fully_connect.shape))\n",
    "    elif activate_func == tf.nn.relu:\n",
    "        ### 2 way\n",
    "        #fully_connect = tf.contrib.layers.fully_connected(tensor_in, num_outputs= node_out,activation_fn= activate_func)\n",
    "        fully_connect = tf.nn.relu(tf.add(tf.matmul(tensor_in,w), b))\n",
    "        ###\n",
    "        print(\"fully_connect\\t\\t\\t\\t%d\\t| %s\" % (fully_connect.shape[1],fully_connect.shape))\n",
    "        print(\"ReLU\\t\\t\\t\\t\\t%d\\t| %s\" % (fully_connect.shape[1],fully_connect.shape))\n",
    "    elif activate_func == tf.nn.softmax:\n",
    "        ### 2 way\n",
    "        #fully_connect = tf.contrib.layers.fully_connected(tensor_in, num_outputs= node_out,activation_fn= activate_func)\n",
    "        fully_connect = tf.nn.softmax(tf.add(tf.matmul(tensor_in,w), b))\n",
    "        ###\n",
    "        print(\"fully_connect\\t\\t\\t\\t%d\\t| %s\" % (fully_connect.shape[1],fully_connect.shape))\n",
    "        print(\"Softmax\\t\\t\\t\\t\\t%d\\t| %s\" % (fully_connect.shape[1],fully_connect.shape))\n",
    "    \n",
    "    return fully_connect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten and NN layer -- about nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tnode\n",
      "flatten\t\t\t\t\t16384\t| (?, 16384)\n",
      "fully_connect\t\t\t\t100\t| (?, 100)\n",
      "fully_connect\t\t\t\t50\t| (?, 50)\n",
      "ReLU\t\t\t\t\t50\t| (?, 50)\n",
      "fully_connect\t\t\t\t10\t| (?, 10)\n",
      "Softmax\t\t\t\t\t10\t| (?, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t\\t\\t\\t\\tnode\")\n",
    "#flat = tf.contrib.layers.flatten(conv2)\n",
    "flat = tf.reshape(conv3, [-1, int(conv3.shape[1])*int(conv3.shape[2])*int(conv3.shape[3])])\n",
    "print(\"flatten\\t\\t\\t\\t\\t%d\\t| %s\" %(flat.shape[1],flat.shape))\n",
    "\n",
    "### Tunning part:\n",
    "nn1_node = 100\n",
    "nn2_node = 50\n",
    "output_class = 10\n",
    "###\n",
    "\n",
    "fc_layer1 = fc_block(flat, nn1_node, None)\n",
    "fc_layer2 = fc_block(fc_layer1, nn2_node, tf.nn.relu)\n",
    "predicted_prob = fc_block(fc_layer1, output_class, tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 4: Expected Output **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            flatten | (?, 128)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            fully_connect | (?, 256)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            fully_connect | (?, 512)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ReLU | (?, 512)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            fully_connect | (?,64)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            softmax | (?, 5)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex.2-1 Choose the cost function\n",
    "## Discussion:\n",
    "> ## 1. Check how the cost function looks like on the Internet, and explain how the weight changes.\n",
    ">> ## Link for mode = 2: [Wolfram Alpha](http://www.wolframalpha.com/input/?i=2*w%5E4+%2B+4*w%5E3+%2B+1*w%5E2)\n",
    "\n",
    "> ## 2. What is cost function?\n",
    "> ## 3. Why do we need to do optimization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose initial weight, and select a cost function (\"shift + enter\" the blocks to the end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(1)\n",
    "x = tf.placeholder(tf.float32,[None,1])\n",
    "\n",
    "### Try w=0,random_uniform\n",
    "w = tf.Variable(0,dtype=tf.float32)\n",
    "#w = tf.Variable(tf.random_uniform([1], seed=2),dtype=tf.float32)\n",
    "###\n",
    "\n",
    "### change different cost function to mode = 1,2,3 \n",
    "mode = 1\n",
    "###\n",
    "\n",
    "if mode == 1:\n",
    "    coefficients = np.array([[1.],[-10.],[25.]])\n",
    "    # operator overloading\n",
    "    # cost = tf.add(tf.add(w**2,tf.multiply(-10.,w),25))\n",
    "    cost = x[0]*w**2 + x[1]*w + x[2]\n",
    "elif mode == 2:\n",
    "    coefficients = np.array([[2.],[4.],[1.]])\n",
    "    cost = x[0]*(w**4) + x[1]*(w**3) + x[2]*(w**2)\n",
    "elif mode == 3:\n",
    "    coefficients = np.array([[5.],[8.],[3.],[1.]])\n",
    "    cost = x[0]*(w**6)+ x[1]*(w**5)+ x[2]*(w**2)+ x[3]*w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex.2-2 Tune the learning rate\n",
    "## Discussion:\n",
    "> ## 1. How does the learning rate affect the weight?\n",
    ">> ## ~~We need to focus on the slope of cost function~~\n",
    "\n",
    "> ## 2. Do you know where's the problem in gradient descent optimization?\n",
    ">> ## a. ~~Local minimum~~\n",
    ">> ## b. ~~Saddle point~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "lr=0.010000\tweight\t\tvalue update\n",
      "--------------------------------------------\n",
      "Initialize:\t0.000000\n",
      "Epoch 1:\t0.100000\t0.100000\n",
      "Epoch 2:\t0.198000\t0.098000\n",
      "Epoch 3:\t0.294040\t0.096040\n",
      "Epoch 4:\t0.388159\t0.094119\n",
      "Epoch 5:\t0.480396\t0.092237\n",
      "Epoch 6:\t0.570788\t0.090392\n",
      "Epoch 7:\t0.659372\t0.088584\n",
      "Epoch 8:\t0.746185\t0.086813\n",
      "Epoch 9:\t0.831261\t0.085076\n",
      "Epoch 10:\t0.914636\t0.083375\n",
      "Epoch 11:\t0.996343\t0.081707\n",
      "Epoch 12:\t1.076416\t0.080073\n",
      "Epoch 13:\t1.154888\t0.078472\n",
      "Epoch 14:\t1.231790\t0.076902\n",
      "Epoch 15:\t1.307155\t0.075364\n",
      "Epoch 16:\t1.381011\t0.073857\n",
      "Epoch 17:\t1.453391\t0.072380\n",
      "Epoch 18:\t1.524323\t0.070932\n",
      "Epoch 19:\t1.593837\t0.069514\n",
      "Epoch 20:\t1.661960\t0.068123\n",
      "Epoch 21:\t1.728721\t0.066761\n",
      "Epoch 22:\t1.794147\t0.065426\n",
      "Epoch 23:\t1.858264\t0.064117\n",
      "Epoch 24:\t1.921098\t0.062835\n",
      "Epoch 25:\t1.982676\t0.061578\n",
      "Epoch 26:\t2.043023\t0.060346\n",
      "Epoch 27:\t2.102162\t0.059139\n",
      "Epoch 28:\t2.160119\t0.057957\n",
      "Epoch 29:\t2.216917\t0.056798\n",
      "Epoch 30:\t2.272578\t0.055662\n",
      "Epoch 31:\t2.327127\t0.054549\n",
      "Epoch 32:\t2.380584\t0.053457\n",
      "Epoch 33:\t2.432973\t0.052388\n",
      "Epoch 34:\t2.484313\t0.051341\n",
      "Epoch 35:\t2.534627\t0.050314\n",
      "Epoch 36:\t2.583934\t0.049307\n",
      "Epoch 37:\t2.632256\t0.048321\n",
      "Epoch 38:\t2.679610\t0.047355\n",
      "Epoch 39:\t2.726018\t0.046408\n",
      "Epoch 40:\t2.771498\t0.045480\n",
      "Epoch 41:\t2.816068\t0.044570\n",
      "Epoch 42:\t2.859746\t0.043679\n",
      "Epoch 43:\t2.902551\t0.042805\n",
      "Epoch 44:\t2.944500\t0.041949\n",
      "Epoch 45:\t2.985610\t0.041110\n",
      "Epoch 46:\t3.025898\t0.040288\n",
      "Epoch 47:\t3.065380\t0.039482\n",
      "Epoch 48:\t3.104073\t0.038692\n",
      "Epoch 49:\t3.141991\t0.037919\n",
      "Epoch 50:\t3.179152\t0.037160\n",
      "Epoch 51:\t3.215569\t0.036417\n",
      "Epoch 52:\t3.251257\t0.035689\n",
      "Epoch 53:\t3.286232\t0.034975\n",
      "Epoch 54:\t3.320507\t0.034275\n",
      "Epoch 55:\t3.354097\t0.033590\n",
      "Epoch 56:\t3.387015\t0.032918\n",
      "Epoch 57:\t3.419275\t0.032260\n",
      "Epoch 58:\t3.450889\t0.031615\n",
      "Epoch 59:\t3.481872\t0.030982\n",
      "Epoch 60:\t3.512234\t0.030363\n",
      "Epoch 61:\t3.541990\t0.029755\n",
      "Epoch 62:\t3.571150\t0.029160\n",
      "Epoch 63:\t3.599727\t0.028577\n",
      "Epoch 64:\t3.627732\t0.028005\n",
      "Epoch 65:\t3.655178\t0.027445\n",
      "Epoch 66:\t3.682074\t0.026896\n",
      "Epoch 67:\t3.708433\t0.026359\n",
      "Epoch 68:\t3.734264\t0.025831\n",
      "Epoch 69:\t3.759579\t0.025315\n",
      "Epoch 70:\t3.784387\t0.024808\n",
      "Epoch 71:\t3.808700\t0.024312\n",
      "Epoch 72:\t3.832526\t0.023826\n",
      "Epoch 73:\t3.855875\t0.023350\n",
      "Epoch 74:\t3.878758\t0.022882\n",
      "Epoch 75:\t3.901183\t0.022425\n",
      "Epoch 76:\t3.923159\t0.021976\n",
      "Epoch 77:\t3.944696\t0.021537\n",
      "Epoch 78:\t3.965802\t0.021106\n",
      "Epoch 79:\t3.986486\t0.020684\n",
      "Epoch 80:\t4.006756\t0.020270\n",
      "Epoch 81:\t4.026621\t0.019865\n",
      "Epoch 82:\t4.046088\t0.019467\n",
      "Epoch 83:\t4.065166\t0.019078\n",
      "Epoch 84:\t4.083863\t0.018697\n",
      "Epoch 85:\t4.102186\t0.018323\n",
      "Epoch 86:\t4.120142\t0.017956\n",
      "Epoch 87:\t4.137740\t0.017597\n",
      "Epoch 88:\t4.154985\t0.017245\n",
      "Epoch 89:\t4.171885\t0.016901\n",
      "Epoch 90:\t4.188448\t0.016562\n",
      "Epoch 91:\t4.204679\t0.016231\n",
      "Epoch 92:\t4.220585\t0.015906\n",
      "Epoch 93:\t4.236174\t0.015588\n",
      "Epoch 94:\t4.251450\t0.015276\n",
      "Epoch 95:\t4.266421\t0.014971\n",
      "Epoch 96:\t4.281093\t0.014672\n",
      "Epoch 97:\t4.295471\t0.014378\n",
      "Epoch 98:\t4.309561\t0.014091\n",
      "Epoch 99:\t4.323370\t0.013809\n",
      "Epoch 100:\t4.336903\t0.013533\n"
     ]
    }
   ],
   "source": [
    "### play for the value between: 0.9 ~ 0.01\n",
    "learning_rate = 0.01\n",
    "###\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"lr=%f\\tweight\\t\\tvalue update\" %learning_rate)\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    print(\"Initialize:\\t%f\" %session.run(w))\n",
    "    wf = session.run(w)\n",
    "    for i in range(100):\n",
    "        # feed_dict={x:coefficients}, feed our data \"coefficients\" into our placeholder \"x\"\n",
    "        session.run(train,feed_dict={x:coefficients})\n",
    "        print(\"Epoch %d:\\t%f\\t%f\" %(i+1,session.run(w),session.run(w)-wf))\n",
    "        wf = session.run(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
