{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics\n",
    "> ## 1. Build a model\n",
    ">> ### 1.1 Create placeholder\n",
    ">>> #### Coding 1: Create integer placeholder\n",
    "\n",
    ">> ### 1.2 One-hot encoding\n",
    ">> ### 1.3 Define the weight of CNN\n",
    ">> ### 1.4 Understanding the blocks/layers\n",
    ">>> #### Coding 2: Add some layers in the model\n",
    "\n",
    "\n",
    "> ## 2. Optimization -- Gradient Descent\n",
    ">> ### 2.1 Loss function and Learning rate\n",
    ">>> #### Coding 3: Tune the learning rate on loss function\n",
    "\n",
    ">> ### 2.2 Minibatch\n",
    "\n",
    "> ## 3. Pre-trained model -- YOLOv3, ResNet (jupyter_2)\n",
    "> ## 4. Training small dataset, using Tensorboard (jupyter_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build a model\n",
    "## 1.1 Create placeholder: \n",
    "> ## The pre-allocate dimensions for input and output\n",
    "![Placeholder](https://imgur.com/lTf4ehx.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_H0, n_W0, n_C0, n_y, n_state):\n",
    "    # Create float placeholder on input_picture\n",
    "    X = tf.placeholder(tf.float32,[None,n_H0,n_W0,n_C0])\n",
    "    # Create integer placeholder on output_class\n",
    "    Y = tf.placeholder(tf.int32,[None,n_y])\n",
    "    \n",
    "    ### Coding 1: Create integer placeholder on input_state\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    Z = tf.placeholder(tf.int32,[None,n_state])\n",
    "    #Z = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_height = 256 #64 #32\n",
    "pic_width = 256 #64 #32\n",
    "pic_channel = 3\n",
    "output_class = 6\n",
    "input_state = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pic_placehold = Tensor(\"Placeholder:0\", shape=(?, 256, 256, 3), dtype=float32)\n",
      "action_placehold = Tensor(\"Placeholder_1:0\", shape=(?, 6), dtype=int32)\n",
      "state_placehold = Tensor(\"Placeholder_2:0\", shape=(?, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "pic_placehold, action_placehold, state_placehold = create_placeholders(pic_height, pic_width, pic_channel, output_class, input_state)\n",
    "print (\"pic_placehold = \" + str(pic_placehold))\n",
    "print (\"action_placehold = \" + str(action_placehold))\n",
    "print (\"state_placehold = \" + str(state_placehold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "<table> \n",
    "<tr>\n",
    "<td>\n",
    "    pic_placehold = Tensor(\"Placeholder:0\", shape=(?, 256, 256, 3), dtype=float32)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "    action_placehold  = Tensor(\"Placeholder_1:0\", shape=(?, 6), dtype=int32)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>\n",
    "    state_placehold  = Tensor(\"Placeholder_2:0\", shape=(?, 4), dtype=int32)\n",
    "        \n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 One-hot encoding\n",
    "> ### ![one-hot](https://imgur.com/jCjGQgi.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action = {0:\"right\", 1:\"left\", 2:\"front\", 3:\"back\", 4:\"up\", 5:\"down\"}\n",
    "action_type = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_onehot = tf.one_hot(action_placehold , action_type)\n",
    "#print(action_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Define the weight of CNN: \n",
    "> ### Filter size and Channel (Create initialized parameters)\n",
    ">> ## Filter example\n",
    "![Sobel filter](https://imgur.com/YaAq4q7.png)\n",
    ">> ## After filter\n",
    "![Human](https://imgur.com/C8XqlHm.png)\n",
    ">> ## Channel\n",
    "![RGB](https://imgur.com/ygKlPRO.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    # so that your \"random\" numbers match ours\n",
    "    tf.set_random_seed(1)                              \n",
    "    \n",
    "    # store weight\n",
    "    # CNN part operation\n",
    "    # filter_width:4,filter_height:4,input_channel:3,output_channel:8\n",
    "    W1 = tf.get_variable(\"W1\", [4,4,3,8], initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    \n",
    "    # filter_width:2,filter_height:2,input_channel:8,output_channel:16\n",
    "    W2 = tf.get_variable(\"W2\", [2,2,8,16], initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "\n",
    "    # make it into a dictionary of parameters\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1_first graph_channel values = [ 0.00131723  0.14176141 -0.04434952  0.09197326  0.14984085 -0.03514394\n",
      " -0.06847463  0.05245192]\n",
      "W2_first graph_channel values = [-0.08566415  0.17750949  0.11974221  0.16773748 -0.0830943  -0.08058\n",
      " -0.00577033 -0.14643836  0.24162132 -0.05857408 -0.19055021  0.1345228\n",
      " -0.22779644 -0.1601823  -0.16117483 -0.10286498]\n"
     ]
    }
   ],
   "source": [
    "# have a look inside the weight\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess_test:\n",
    "    parameters = initialize_parameters()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess_test.run(init)\n",
    "    print(\"W1_first graph_channel values = \" + str(parameters[\"W1\"].eval()[1,1,1]))\n",
    "    print(\"W2_first graph_channel values = \" + str(parameters[\"W2\"].eval()[1,1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Output:**\n",
    "\n",
    "<table> \n",
    "\n",
    "    <tr>\n",
    "        <td>\n",
    "        W1_first graph_channel values = \n",
    "        </td>\n",
    "        <td>\n",
    "[ 0.00131723  0.14176141 -0.04434952  0.09197326  0.14984085 -0.03514394 <br>\n",
    " -0.06847463  0.05245192]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "    <tr>\n",
    "        <td>\n",
    "        W2_first graph_channel values = \n",
    "        </td>\n",
    "        <td>\n",
    "[-0.08566415  0.17750949  0.11974221  0.16773748 -0.0830943  -0.08058 <br>\n",
    " -0.00577033 -0.14643836  0.24162132 -0.05857408 -0.19055021  0.1345228 <br>\n",
    " -0.22779644 -0.1601823  -0.16117483 -0.10286498]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Understanding the blocks/layers \n",
    "> ### CNN: strides, channel\n",
    "> ### CNN to NN: Flatten()\n",
    "> ### NN:  Fully_Connected()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Some definitions of functions \n",
    ">> (you can check on Tensorflow for other function used: https://www.tensorflow.org/api_docs/python/tf)\n",
    "\n",
    "> ## Hint for 1.4\n",
    "- ## <span style=\"color:red\">Graphic operations: CNN</span>\n",
    "- ### tf.nn.conv2d()\n",
    "- ### tf.nn.max_pool()\n",
    "- ### tf.contrib.layers.flatten()\n",
    "- ### tf.contrib.layers.fully_connected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(tensor_in, channel_in, channel_out, filter_height, filter_width, strides_height, strides_width, maxPool_height=2, maxPool_width=2):\n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    w = tf.Variable(tf.truncated_normal([filter_height,filter_width,channel_in,channel_out], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[channel_out]))\n",
    "    \n",
    "    conv = tf.nn.conv2d(tensor_in, w, strides=[1, strides_height, strides_width,1], padding='SAME')                \n",
    "    print(\"Conv\\t\\t%d\\t%d\\t%d\\t\\t| %s\" % (conv.shape[1],conv.shape[2],conv.shape[3],conv.shape))\n",
    "    # activate each nodes with bias\n",
    "    activate_func = tf.nn.relu(conv + b)\n",
    "    print(\"ReLU\\t\\t%d\\t%d\\t%d\\t\\t| %s\" % (activate_func.shape[1],activate_func.shape[2],activate_func.shape[3],activate_func.shape))\n",
    "    \n",
    "    #maxPool_height = 2\n",
    "    #maxPool_width = 2\n",
    "    maxPool = tf.nn.max_pool(activate_func, ksize=[1,maxPool_height,maxPool_width,1], strides=[1,maxPool_height,maxPool_width,1], padding='SAME')\n",
    "    print(\"max_pool\\t%d\\t%d\\t%d\\t\\t| %s\" %(maxPool.shape[1],maxPool.shape[2],maxPool.shape[3],maxPool.shape))\n",
    "    return maxPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "Layer\t\tHeight\tWidth\tChannel\tNode\t| What you will see\n",
      "---------------------------------------------------------------\n",
      "Input\t\t256\t256\t3\t\t| (?, 256, 256, 3)\n",
      "Conv\t\t128\t128\t8\t\t| (?, 128, 128, 8)\n",
      "ReLU\t\t128\t128\t8\t\t| (?, 128, 128, 8)\n",
      "max_pool\t64\t64\t8\t\t| (?, 64, 64, 8)\n"
     ]
    }
   ],
   "source": [
    "s = \"---------------------------------------------------------------\"\n",
    "print(s)\n",
    "print(\"Layer\\t\\tHeight\\tWidth\\tChannel\\tNode\\t| What you will see\")\n",
    "print(s)\n",
    "print(\"Input\\t\\t%d\\t%d\\t%d\\t\\t| %s\" %(pic_placehold.shape[1],pic_placehold.shape[2],pic_placehold.shape[3],pic_placehold.shape))\n",
    "\n",
    "### Coding 2: Add some layers in the model\n",
    "### Step 1: Tunning the stride for first CNN layer\n",
    "### Tunning part:\n",
    "strides_height = 2\n",
    "strides_width = 2\n",
    "###\n",
    "conv1 = conv_block(pic_placehold, 3, 8, 4, 4, strides_height, strides_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 1: Expected Output **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Input | (?, 256, 256, 3)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Conv | (?, 128, 128, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ReLU | (?, 128, 128, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            max_pool | (?, 64, 64, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv\t\t16\t32\t16\t\t| (?, 16, 32, 16)\n",
      "ReLU\t\t16\t32\t16\t\t| (?, 16, 32, 16)\n",
      "max_pool\t8\t16\t16\t\t| (?, 8, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "### Step 2: Tunning the stride for second CNN layer + tune the channel\n",
    "### Tunning part: divisible stride\n",
    "strides_height = #4\n",
    "strides_width = #2\n",
    "channel_in = #8\n",
    "channel_out = #16\n",
    "###\n",
    "conv2 = conv_block(conv1, channel_in, channel_out, 4, 4, strides_height, strides_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 2: Expected Output **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Conv | (?, 16, 32, 16)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ReLU | (?, 16, 32, 16)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            max_pool | (?, 8, 16, 16)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv\t\t4\t6\t32\t\t| (?, 4, 6, 32)\n",
      "ReLU\t\t4\t6\t32\t\t| (?, 4, 6, 32)\n",
      "max_pool\t2\t3\t32\t\t| (?, 2, 3, 32)\n"
     ]
    }
   ],
   "source": [
    "### Step 3: Tunning the stride + build the third layer of CNN\n",
    "### Tunning part: non-divisible stride\n",
    "strides_height = #2\n",
    "strides_width = #3\n",
    "###\n",
    "#conv3 = conv_block(conv2, 16, 32, 4, 4, strides_height, strides_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 3: Expected Output **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Conv | (?, 4, 6, 32)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ReLU | (?, 4, 6, 32)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            max_pool | (?, 2, 3, 32)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "### 1. How does \"Step 1\", \"Step 2\", \"Step 3\" happens?\n",
    "> #### CNN operation\n",
    ">> ![CNN](https://imgur.com/FIy5Ou4.gif)\n",
    "\n",
    "> #### Max Pooling operation (Vote for the largest number)\n",
    ">> ![Max Pool](https://imgur.com/ec0zNkC.png)\n",
    "\n",
    "### 2. Why do we put pic_placehold in \"Step 1\" conv_block?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 給一個 3層對的範例\n",
    "\n",
    "# channel checkpoint: 5層之間亂給，固定 2,4層，希望 input是多少，output是多少\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_block(tensor_in, node_out, activate_func):\n",
    "    # We do not care the reusable or not now, so we do not give variable a name\n",
    "    node_in = int(tensor_in.shape[1])\n",
    "    w = tf.Variable(tf.random_normal([node_in, node_out], stddev=0.35))\n",
    "    b = tf.Variable(tf.zeros([node_out]))\n",
    "    \n",
    "    if activate_func == None:\n",
    "        ### 2 way\n",
    "        #fully_connect = tf.contrib.layers.fully_connected(tensor_in, num_outputs= node_out,activation_fn= None)\n",
    "        fully_connect = tf.add(tf.matmul(tensor_in,w), b)\n",
    "        ###\n",
    "        print(\"fully_connect\\t\\t\\t\\t%d\\t| %s\" % (fully_connect.shape[1],fully_connect.shape))\n",
    "    elif activate_func == tf.nn.relu:\n",
    "        ### 2 way\n",
    "        #fully_connect = tf.contrib.layers.fully_connected(tensor_in, num_outputs= node_out,activation_fn= activate_func)\n",
    "        fully_connect = tf.nn.relu(tf.add(tf.matmul(tensor_in,w), b))\n",
    "        ###\n",
    "        print(\"fully_connect\\t\\t\\t\\t%d\\t| %s\" % (fully_connect.shape[1],fully_connect.shape))\n",
    "        print(\"ReLU\\t\\t\\t\\t\\t%d\\t| %s\" % (fully_connect.shape[1],fully_connect.shape))\n",
    "    elif activate_func == tf.nn.softmax:\n",
    "        ### 2 way\n",
    "        #fully_connect = tf.contrib.layers.fully_connected(tensor_in, num_outputs= node_out,activation_fn= activate_func)\n",
    "        fully_connect = tf.nn.softmax(tf.add(tf.matmul(tensor_in,w), b))\n",
    "        ###\n",
    "        print(\"fully_connect\\t\\t\\t\\t%d\\t| %s\" % (fully_connect.shape[1],fully_connect.shape))\n",
    "        print(\"Softmax\\t\\t\\t\\t\\t%d\\t| %s\" % (fully_connect.shape[1],fully_connect.shape))\n",
    "    \n",
    "    return fully_connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "Layer\t\tHeight\tWidth\tChannel\tNode\t| What you will see\n",
      "---------------------------------------------------------------\n",
      "Input\t\t256\t256\t3\t\t| (?, 256, 256, 3)\n",
      "Conv\t\t128\t128\t16\t\t| (?, 128, 128, 16)\n",
      "ReLU\t\t128\t128\t16\t\t| (?, 128, 128, 16)\n",
      "max_pool\t32\t128\t16\t\t| (?, 32, 128, 16)\n",
      "Conv\t\t32\t128\t64\t\t| (?, 32, 128, 64)\n",
      "ReLU\t\t32\t128\t64\t\t| (?, 32, 128, 64)\n",
      "max_pool\t16\t64\t64\t\t| (?, 16, 64, 64)\n",
      "flatten\t\t\t\t\t65536\t| (?, 65536)\n",
      "fully_connect\t\t\t\t256\t| (?, 256)\n",
      "fully_connect\t\t\t\t128\t| (?, 128)\n",
      "ReLU\t\t\t\t\t128\t| (?, 128)\n",
      "fully_connect\t\t\t\t6\t| (?, 6)\n",
      "Softmax\t\t\t\t\t6\t| (?, 6)\n"
     ]
    }
   ],
   "source": [
    "# Mix with CNN, NN\n",
    "s = \"---------------------------------------------------------------\"\n",
    "print(s)\n",
    "print(\"Layer\\t\\tHeight\\tWidth\\tChannel\\tNode\\t| What you will see\")\n",
    "print(s)\n",
    "print(\"Input\\t\\t%d\\t%d\\t%d\\t\\t| %s\" %(pic_placehold.shape[1],pic_placehold.shape[2],pic_placehold.shape[3],pic_placehold.shape))\n",
    "\n",
    "### Step 4: Select max_pool size + Build 2 CNN layer + Select any filter size\n",
    "#conv1 = conv_block(pic_placehold, 3, 16, 4, 4, 2, 2, 4, 1)\n",
    "#conv2 = conv_block(conv1, 16, 64, 2, 2, 1, 1)\n",
    "###\n",
    "\n",
    "#flat = tf.contrib.layers.flatten(conv2)\n",
    "flat = tf.reshape(conv2, [-1, int(conv2.shape[1])*int(conv2.shape[2])*int(conv2.shape[3])])\n",
    "print(\"flatten\\t\\t\\t\\t\\t%d\\t| %s\" %(flat.shape[1],flat.shape))\n",
    "\n",
    "fc_layer1 = fc_block(flat, 256, None)\n",
    "fc_layer2 = fc_block(fc_layer1, 128, tf.nn.relu)\n",
    "predicted_prob = fc_block(fc_layer1, output_class, tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 4: Expected Output **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Input | (?, 256, 256, 3)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Conv | (?, 128, 128, 16)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ReLU | (?, 128, 128, 16)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            max_pool | (?, 32, 128, 16)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Conv | (?, 32, 128, 64)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ReLU | (?, 32, 128, 64)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            max_pool | (?, 16, 64, 64)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Optimization -- Gradient Descent\n",
    "## 2.1 Loss function and Learning rate\n",
    "> ### Gradient Descent: update with Learning_rate*slope\n",
    ">> ### 1. Loss function: (formula...)\n",
    "![Optimization](https://imgur.com/LXBjfLb.png)\n",
    ">> ### 2. Learning rate\n",
    "![Learning rate](https://imgur.com/J8U8fu9.jpg)\n",
    "\n",
    "> ## Meaning of loss function\n",
    ">> ### \"預測\"和\"實際\"誤差\n",
    "\n",
    "> ## Optimization Problems\n",
    ">> ### 1. 局部極小值\n",
    ">> ### 2. Saddle point\n",
    "\n",
    "> ## Overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = np.array([[1.],[-10.],[25.]])\n",
    "w = tf.Variable(0,dtype=tf.float32)\n",
    "x = tf.placeholder(tf.float32,[3,1])\n",
    "\n",
    "# cost = tf.add(tf.add(w**2,tf.multiply(-10.,w),25))\n",
    "# operator overloading\n",
    "# placeholder x, input\n",
    "cost = x[0]*w**2 + x[1]*w + x[2]\n",
    "#[1,2]\n",
    "#[2,4,1]\n",
    "#cost = x[0]*w**4 + x[1]*w**3 + x[2]*w**2\n",
    "#[5,8,3,1]\n",
    "#cost = x[0]*w**6+ x[1]*w**5+ x[2]*w**2+ x[3]*w\n",
    "# broadcasting\n",
    "#cost = w*x # (1*3)*(3*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "lr=0.010000\tweight\t\tvalue update\n",
      "--------------------------------------------\n",
      "Initialize:\t0.000000\n",
      "Epoch 1:\t0.100000\t0.100000\n",
      "Epoch 2:\t0.198000\t0.098000\n",
      "Epoch 3:\t0.294040\t0.096040\n",
      "Epoch 4:\t0.388159\t0.094119\n",
      "Epoch 5:\t0.480396\t0.092237\n",
      "Epoch 6:\t0.570788\t0.090392\n",
      "Epoch 7:\t0.659372\t0.088584\n",
      "Epoch 8:\t0.746185\t0.086813\n",
      "Epoch 9:\t0.831261\t0.085076\n",
      "Epoch 10:\t0.914636\t0.083375\n",
      "Epoch 11:\t0.996343\t0.081707\n",
      "Epoch 12:\t1.076416\t0.080073\n",
      "Epoch 13:\t1.154888\t0.078472\n",
      "Epoch 14:\t1.231790\t0.076902\n",
      "Epoch 15:\t1.307155\t0.075364\n",
      "Epoch 16:\t1.381011\t0.073857\n",
      "Epoch 17:\t1.453391\t0.072380\n",
      "Epoch 18:\t1.524323\t0.070932\n",
      "Epoch 19:\t1.593837\t0.069514\n",
      "Epoch 20:\t1.661960\t0.068123\n",
      "Epoch 21:\t1.728721\t0.066761\n",
      "Epoch 22:\t1.794147\t0.065426\n",
      "Epoch 23:\t1.858264\t0.064117\n",
      "Epoch 24:\t1.921098\t0.062835\n",
      "Epoch 25:\t1.982676\t0.061578\n",
      "Epoch 26:\t2.043023\t0.060346\n",
      "Epoch 27:\t2.102162\t0.059139\n",
      "Epoch 28:\t2.160119\t0.057957\n",
      "Epoch 29:\t2.216917\t0.056798\n",
      "Epoch 30:\t2.272578\t0.055662\n",
      "Epoch 31:\t2.327127\t0.054549\n",
      "Epoch 32:\t2.380584\t0.053457\n",
      "Epoch 33:\t2.432973\t0.052388\n",
      "Epoch 34:\t2.484313\t0.051341\n",
      "Epoch 35:\t2.534627\t0.050314\n",
      "Epoch 36:\t2.583934\t0.049307\n",
      "Epoch 37:\t2.632256\t0.048321\n",
      "Epoch 38:\t2.679610\t0.047355\n",
      "Epoch 39:\t2.726018\t0.046408\n",
      "Epoch 40:\t2.771498\t0.045480\n",
      "Epoch 41:\t2.816068\t0.044570\n",
      "Epoch 42:\t2.859746\t0.043679\n",
      "Epoch 43:\t2.902551\t0.042805\n",
      "Epoch 44:\t2.944500\t0.041949\n",
      "Epoch 45:\t2.985610\t0.041110\n",
      "Epoch 46:\t3.025898\t0.040288\n",
      "Epoch 47:\t3.065380\t0.039482\n",
      "Epoch 48:\t3.104073\t0.038692\n",
      "Epoch 49:\t3.141991\t0.037919\n",
      "Epoch 50:\t3.179152\t0.037160\n",
      "Epoch 51:\t3.215569\t0.036417\n",
      "Epoch 52:\t3.251257\t0.035689\n",
      "Epoch 53:\t3.286232\t0.034975\n",
      "Epoch 54:\t3.320507\t0.034275\n",
      "Epoch 55:\t3.354097\t0.033590\n",
      "Epoch 56:\t3.387015\t0.032918\n",
      "Epoch 57:\t3.419275\t0.032260\n",
      "Epoch 58:\t3.450889\t0.031615\n",
      "Epoch 59:\t3.481872\t0.030982\n",
      "Epoch 60:\t3.512234\t0.030363\n",
      "Epoch 61:\t3.541990\t0.029755\n",
      "Epoch 62:\t3.571150\t0.029160\n",
      "Epoch 63:\t3.599727\t0.028577\n",
      "Epoch 64:\t3.627732\t0.028005\n",
      "Epoch 65:\t3.655178\t0.027445\n",
      "Epoch 66:\t3.682074\t0.026896\n",
      "Epoch 67:\t3.708433\t0.026359\n",
      "Epoch 68:\t3.734264\t0.025831\n",
      "Epoch 69:\t3.759579\t0.025315\n",
      "Epoch 70:\t3.784387\t0.024808\n",
      "Epoch 71:\t3.808700\t0.024312\n",
      "Epoch 72:\t3.832526\t0.023826\n",
      "Epoch 73:\t3.855875\t0.023350\n",
      "Epoch 74:\t3.878758\t0.022882\n",
      "Epoch 75:\t3.901183\t0.022425\n",
      "Epoch 76:\t3.923159\t0.021976\n",
      "Epoch 77:\t3.944696\t0.021537\n",
      "Epoch 78:\t3.965802\t0.021106\n",
      "Epoch 79:\t3.986486\t0.020684\n",
      "Epoch 80:\t4.006756\t0.020270\n",
      "Epoch 81:\t4.026621\t0.019865\n",
      "Epoch 82:\t4.046088\t0.019467\n",
      "Epoch 83:\t4.065166\t0.019078\n",
      "Epoch 84:\t4.083863\t0.018697\n",
      "Epoch 85:\t4.102186\t0.018323\n",
      "Epoch 86:\t4.120142\t0.017956\n",
      "Epoch 87:\t4.137740\t0.017597\n",
      "Epoch 88:\t4.154985\t0.017245\n",
      "Epoch 89:\t4.171885\t0.016901\n",
      "Epoch 90:\t4.188448\t0.016562\n",
      "Epoch 91:\t4.204679\t0.016231\n",
      "Epoch 92:\t4.220585\t0.015906\n",
      "Epoch 93:\t4.236174\t0.015588\n",
      "Epoch 94:\t4.251450\t0.015276\n",
      "Epoch 95:\t4.266421\t0.014971\n",
      "Epoch 96:\t4.281093\t0.014672\n",
      "Epoch 97:\t4.295471\t0.014378\n",
      "Epoch 98:\t4.309561\t0.014091\n",
      "Epoch 99:\t4.323370\t0.013809\n",
      "Epoch 100:\t4.336903\t0.013533\n"
     ]
    }
   ],
   "source": [
    "# play for the value between: 0.9 ~ 0.01\n",
    "learning_rate = 0.01\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "#first way\n",
    "'''\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "print(session.run(w))\n",
    "'''\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"lr=%f\\tweight\\t\\tvalue update\" %learning_rate)\n",
    "print(\"--------------------------------------------\")\n",
    "#second way\n",
    "with tf.Session() as session:\n",
    "    # initialize\n",
    "    session.run(init)\n",
    "    print(\"Initialize:\\t%f\" %session.run(w))\n",
    "    wf = session.run(w)\n",
    "    #session.run(train,feed_dict={x:coefficients})\n",
    "    #print(session.run(w))\n",
    "    for i in range(100):\n",
    "        # put input inside\n",
    "        session.run(train,feed_dict={x:coefficients})\n",
    "        # print the weight & 差異, think about the curve & the lowest point\n",
    "        print(\"Epoch %d:\\t%f\\t%f\" %(i+1,session.run(w),session.run(w)-wf))\n",
    "        wf = session.run(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
