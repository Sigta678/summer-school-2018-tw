{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics\n",
    "> ## 1. Build a model\n",
    ">> ### 1.1 Create placeholder\n",
    ">>> #### Coding 1: Create integer placeholder\n",
    "\n",
    ">> ### 1.2 One-hot encoding\n",
    ">> ### 1.3 Define the weight of CNN\n",
    ">> ### 1.4 Understanding the blocks/layers\n",
    ">>> #### Coding 2: Add some layers in the model\n",
    "\n",
    "\n",
    "> ## 2. Optimization -- Gradient Descent\n",
    ">> ### 2.1 Cost function and Learning rate\n",
    ">>> #### Coding 3: Tune the learning rate on cost function\n",
    "\n",
    ">> ### 2.2 Minibatch\n",
    "\n",
    "> ## 3. Pre-trained model -- YOLOv3, ResNet (jupyter_2)\n",
    "> ## 4. Training small dataset, using Tensorboard (jupyter_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build a model\n",
    "## 1.1 Create placeholder: \n",
    "> ## The pre-allocate dimensions for input and output\n",
    "![Placeholder](https://imgur.com/lTf4ehx.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_H0, n_W0, n_C0, n_y, n_state):\n",
    "    # Create float placeholder on input_picture\n",
    "    X = tf.placeholder(tf.float32,[None,n_H0,n_W0,n_C0])\n",
    "    # Create integer placeholder on output_class\n",
    "    Y = tf.placeholder(tf.int32,[None,n_y])\n",
    "    \n",
    "    ### Coding 1: Create integer placeholder on n_state\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    Z = tf.placeholder(tf.int32,[None,n_state])\n",
    "    #Z = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X, Y, Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_height = 256 #64 #32\n",
    "pic_width = 256 #64 #32\n",
    "pic_channel = 3\n",
    "output_class = 6\n",
    "input_state = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pic_placehold = Tensor(\"Placeholder:0\", shape=(?, 256, 256, 3), dtype=float32)\n",
      "action_placehold = Tensor(\"Placeholder_1:0\", shape=(?, 6), dtype=int32)\n",
      "state_placehold = Tensor(\"Placeholder_2:0\", shape=(?, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "pic_placehold, action_placehold, state_placehold = create_placeholders(pic_height, pic_width, pic_channel, output_class, input_state)\n",
    "print (\"pic_placehold = \" + str(pic_placehold))\n",
    "print (\"action_placehold = \" + str(action_placehold))\n",
    "print (\"state_placehold = \" + str(state_placehold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "<table> \n",
    "<tr>\n",
    "<td>\n",
    "    pic_placehold = Tensor(\"Placeholder:0\", shape=(?, 256, 256, 3), dtype=float32)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "    action_placehold  = Tensor(\"Placeholder_1:0\", shape=(?, 6), dtype=int32)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>\n",
    "    state_placehold  = Tensor(\"Placeholder_2:0\", shape=(?, 4), dtype=int32)\n",
    "        \n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 One-hot encoding\n",
    "> ### ![one-hot](https://imgur.com/jCjGQgi.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action = {0:\"right\", 1:\"left\", 2:\"front\", 3:\"back\", 4:\"up\", 5:\"down\"}\n",
    "action_type = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_onehot = tf.one_hot(action_placehold , action_type)\n",
    "#print(action_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Define the weight of CNN: \n",
    "> ### Filter size and Channel (Create initialized parameters)\n",
    ">> ## Filter example\n",
    "![Sobel filter](https://imgur.com/YaAq4q7.png)\n",
    ">> ## After filter\n",
    "![Human](https://imgur.com/C8XqlHm.png)\n",
    ">> ## Channel\n",
    "![RGB](https://imgur.com/ygKlPRO.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    # so that your \"random\" numbers match ours\n",
    "    tf.set_random_seed(1)                              \n",
    "    \n",
    "    # store weight\n",
    "    # CNN part operation\n",
    "    # filter_width:4,filter_height:4,input_channel:3,output_channel:8\n",
    "    W1 = tf.get_variable(\"W1\", [4,4,3,8], initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "    \n",
    "    # filter_width:2,filter_height:2,input_channel:8,output_channel:16\n",
    "    W2 = tf.get_variable(\"W2\", [2,2,8,16], initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "\n",
    "    # make it into a dictionary of parameters\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"W2\": W2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1_first graph_channel values = [ 0.00131723  0.14176141 -0.04434952  0.09197326  0.14984085 -0.03514394\n",
      " -0.06847463  0.05245192]\n",
      "W2_first graph_channel values = [-0.08566415  0.17750949  0.11974221  0.16773748 -0.0830943  -0.08058\n",
      " -0.00577033 -0.14643836  0.24162132 -0.05857408 -0.19055021  0.1345228\n",
      " -0.22779644 -0.1601823  -0.16117483 -0.10286498]\n"
     ]
    }
   ],
   "source": [
    "# have a look inside the weight\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess_test:\n",
    "    parameters = initialize_parameters()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess_test.run(init)\n",
    "    print(\"W1_first graph_channel values = \" + str(parameters[\"W1\"].eval()[1,1,1]))\n",
    "    print(\"W2_first graph_channel values = \" + str(parameters[\"W2\"].eval()[1,1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Expected Output:**\n",
    "\n",
    "<table> \n",
    "\n",
    "    <tr>\n",
    "        <td>\n",
    "        W1_first graph_channel values = \n",
    "        </td>\n",
    "        <td>\n",
    "[ 0.00131723  0.14176141 -0.04434952  0.09197326  0.14984085 -0.03514394 <br>\n",
    " -0.06847463  0.05245192]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "    <tr>\n",
    "        <td>\n",
    "        W2_first graph_channel values = \n",
    "        </td>\n",
    "        <td>\n",
    "[-0.08566415  0.17750949  0.11974221  0.16773748 -0.0830943  -0.08058 <br>\n",
    " -0.00577033 -0.14643836  0.24162132 -0.05857408 -0.19055021  0.1345228 <br>\n",
    " -0.22779644 -0.1601823  -0.16117483 -0.10286498]\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Understanding the blocks/layers \n",
    "> ### CNN: strides, channel\n",
    "> ### CNN to NN: Flatten()\n",
    "> ### NN:  Fully_Connected()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Some definitions of functions \n",
    ">> (you can check on Tensorflow for other function used: https://www.tensorflow.org/api_docs/python/tf)\n",
    "\n",
    "> ## Hint for 1.4\n",
    "- ## <span style=\"color:red\">Graphic operations: CNN</span>\n",
    "- ### tf.nn.conv2d()\n",
    "- ### tf.nn.max_pool()\n",
    "- ### tf.contrib.layers.flatten()\n",
    "- ### tf.contrib.layers.fully_connected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(tensor_in, channel_in, channel_out, filter_height, filter_width, strides_height, strides_width, maxPool_height=2, maxPool_width=2):\n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    w = tf.Variable(tf.truncated_normal([filter_height,filter_width,channel_in,channel_out], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[channel_out]))\n",
    "    \n",
    "    conv = tf.nn.conv2d(tensor_in, w, strides=[1, strides_height, strides_width,1], padding='SAME')                \n",
    "    print(\"Conv\\t\\t%d\\t%d\\t%d\\t\\t| %s\" % (conv.shape[1],conv.shape[2],conv.shape[3],conv.shape))\n",
    "    # activate each nodes with bias\n",
    "    activate_func = tf.nn.relu(conv + b)\n",
    "    print(\"ReLU\\t\\t%d\\t%d\\t%d\\t\\t| %s\" % (activate_func.shape[1],activate_func.shape[2],activate_func.shape[3],activate_func.shape))\n",
    "    \n",
    "    #maxPool_height = 2\n",
    "    #maxPool_width = 2\n",
    "    maxPool = tf.nn.max_pool(activate_func, ksize=[1,maxPool_height,maxPool_width,1], strides=[1,maxPool_height,maxPool_width,1], padding='SAME')\n",
    "    print(\"max_pool\\t%d\\t%d\\t%d\\t\\t| %s\" %(maxPool.shape[1],maxPool.shape[2],maxPool.shape[3],maxPool.shape))\n",
    "    return maxPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "Layer\t\tHeight\tWidth\tChannel\tNode\t| What you will see\n",
      "---------------------------------------------------------------\n",
      "Input\t\t256\t256\t3\t\t| (?, 256, 256, 3)\n",
      "Conv\t\t128\t128\t8\t\t| (?, 128, 128, 8)\n",
      "ReLU\t\t128\t128\t8\t\t| (?, 128, 128, 8)\n",
      "max_pool\t64\t64\t8\t\t| (?, 64, 64, 8)\n"
     ]
    }
   ],
   "source": [
    "s = \"---------------------------------------------------------------\"\n",
    "print(s)\n",
    "print(\"Layer\\t\\tHeight\\tWidth\\tChannel\\tNode\\t| What you will see\")\n",
    "print(s)\n",
    "print(\"Input\\t\\t%d\\t%d\\t%d\\t\\t| %s\" %(pic_placehold.shape[1],pic_placehold.shape[2],pic_placehold.shape[3],pic_placehold.shape))\n",
    "\n",
    "### Coding 2: Add some layers in the model\n",
    "### Step 1: Tunning the stride for first CNN layer\n",
    "### Tunning part:\n",
    "strides_height = 2\n",
    "strides_width = 2\n",
    "###\n",
    "conv1 = conv_block(pic_placehold, 3, 8, 4, 4, strides_height, strides_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 1: Expected Output **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Input | (?, 256, 256, 3)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Conv | (?, 128, 128, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ReLU | (?, 128, 128, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            max_pool | (?, 64, 64, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv\t\t16\t32\t16\t\t| (?, 16, 32, 16)\n",
      "ReLU\t\t16\t32\t16\t\t| (?, 16, 32, 16)\n",
      "max_pool\t8\t16\t16\t\t| (?, 8, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "### Step 2: Tunning the stride for second CNN layer + tune the channel\n",
    "### Tunning part: divisible stride\n",
    "strides_height = #4\n",
    "strides_width = #2\n",
    "channel_in = #8\n",
    "channel_out = #16\n",
    "###\n",
    "conv2 = conv_block(conv1, channel_in, channel_out, 4, 4, strides_height, strides_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 2: Expected Output **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Conv | (?, 16, 32, 16)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ReLU | (?, 16, 32, 16)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            max_pool | (?, 8, 16, 16)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv\t\t4\t6\t32\t\t| (?, 4, 6, 32)\n",
      "ReLU\t\t4\t6\t32\t\t| (?, 4, 6, 32)\n",
      "max_pool\t2\t3\t32\t\t| (?, 2, 3, 32)\n"
     ]
    }
   ],
   "source": [
    "### Step 3: Tunning the stride + build the third layer of CNN\n",
    "### Tunning part: non-divisible stride\n",
    "strides_height = #2\n",
    "strides_width = #3\n",
    "###\n",
    "#conv3 = conv_block(conv2, 16, 32, 4, 4, strides_height, strides_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 3: Expected Output **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Conv | (?, 4, 6, 32)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ReLU | (?, 4, 6, 32)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            max_pool | (?, 2, 3, 32)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "### 1. How does \"Step 1\", \"Step 2\", \"Step 3\" happens?\n",
    "> #### CNN operation\n",
    ">> ![CNN](https://imgur.com/FIy5Ou4.gif)\n",
    "\n",
    "> #### Max Pooling operation (Vote for the largest number)\n",
    ">> ![Max Pool](https://imgur.com/ec0zNkC.png)\n",
    "\n",
    "### 2. Why do we put pic_placehold in \"Step 1\" conv_block?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_block(tensor_in, node_out, activate_func):\n",
    "    # We do not care the reusable or not now, so we do not give variable a name\n",
    "    node_in = int(tensor_in.shape[1])\n",
    "    w = tf.Variable(tf.random_normal([node_in, node_out], stddev=0.35))\n",
    "    b = tf.Variable(tf.zeros([node_out]))\n",
    "    \n",
    "    if activate_func == None:\n",
    "        ### 2 way\n",
    "        #fully_connect = tf.contrib.layers.fully_connected(tensor_in, num_outputs= node_out,activation_fn= None)\n",
    "        fully_connect = tf.add(tf.matmul(tensor_in,w), b)\n",
    "        ###\n",
    "        print(\"fully_connect\\t\\t\\t\\t%d\\t| %s\" % (fully_connect.shape[1],fully_connect.shape))\n",
    "    elif activate_func == tf.nn.relu:\n",
    "        ### 2 way\n",
    "        #fully_connect = tf.contrib.layers.fully_connected(tensor_in, num_outputs= node_out,activation_fn= activate_func)\n",
    "        fully_connect = tf.nn.relu(tf.add(tf.matmul(tensor_in,w), b))\n",
    "        ###\n",
    "        print(\"fully_connect\\t\\t\\t\\t%d\\t| %s\" % (fully_connect.shape[1],fully_connect.shape))\n",
    "        print(\"ReLU\\t\\t\\t\\t\\t%d\\t| %s\" % (fully_connect.shape[1],fully_connect.shape))\n",
    "    elif activate_func == tf.nn.softmax:\n",
    "        ### 2 way\n",
    "        #fully_connect = tf.contrib.layers.fully_connected(tensor_in, num_outputs= node_out,activation_fn= activate_func)\n",
    "        fully_connect = tf.nn.softmax(tf.add(tf.matmul(tensor_in,w), b))\n",
    "        ###\n",
    "        print(\"fully_connect\\t\\t\\t\\t%d\\t| %s\" % (fully_connect.shape[1],fully_connect.shape))\n",
    "        print(\"Softmax\\t\\t\\t\\t\\t%d\\t| %s\" % (fully_connect.shape[1],fully_connect.shape))\n",
    "    \n",
    "    return fully_connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "Layer\t\tHeight\tWidth\tChannel\tNode\t| What you will see\n",
      "---------------------------------------------------------------\n",
      "Input\t\t256\t256\t3\t\t| (?, 256, 256, 3)\n",
      "Conv\t\t128\t128\t16\t\t| (?, 128, 128, 16)\n",
      "ReLU\t\t128\t128\t16\t\t| (?, 128, 128, 16)\n",
      "max_pool\t32\t128\t16\t\t| (?, 32, 128, 16)\n",
      "Conv\t\t32\t128\t64\t\t| (?, 32, 128, 64)\n",
      "ReLU\t\t32\t128\t64\t\t| (?, 32, 128, 64)\n",
      "max_pool\t16\t64\t64\t\t| (?, 16, 64, 64)\n",
      "flatten\t\t\t\t\t65536\t| (?, 65536)\n",
      "fully_connect\t\t\t\t256\t| (?, 256)\n",
      "fully_connect\t\t\t\t128\t| (?, 128)\n",
      "ReLU\t\t\t\t\t128\t| (?, 128)\n",
      "fully_connect\t\t\t\t6\t| (?, 6)\n",
      "Softmax\t\t\t\t\t6\t| (?, 6)\n"
     ]
    }
   ],
   "source": [
    "# Mix with CNN, NN\n",
    "s = \"---------------------------------------------------------------\"\n",
    "print(s)\n",
    "print(\"Layer\\t\\tHeight\\tWidth\\tChannel\\tNode\\t| What you will see\")\n",
    "print(s)\n",
    "print(\"Input\\t\\t%d\\t%d\\t%d\\t\\t| %s\" %(pic_placehold.shape[1],pic_placehold.shape[2],pic_placehold.shape[3],pic_placehold.shape))\n",
    "\n",
    "### Step 4: Select max_pool size + Build 2 CNN layer + Select any filter size\n",
    "#conv1 = conv_block(pic_placehold, 3, 16, 4, 4, 2, 2, 4, 1)\n",
    "#conv2 = conv_block(conv1, 16, 64, 2, 2, 1, 1)\n",
    "###\n",
    "\n",
    "#flat = tf.contrib.layers.flatten(conv2)\n",
    "flat = tf.reshape(conv2, [-1, int(conv2.shape[1])*int(conv2.shape[2])*int(conv2.shape[3])])\n",
    "print(\"flatten\\t\\t\\t\\t\\t%d\\t| %s\" %(flat.shape[1],flat.shape))\n",
    "\n",
    "fc_layer1 = fc_block(flat, 256, None)\n",
    "fc_layer2 = fc_block(fc_layer1, 128, tf.nn.relu)\n",
    "predicted_prob = fc_block(fc_layer1, output_class, tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 4: Expected Output **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Input | (?, 256, 256, 3)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Conv | (?, 128, 128, 16)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ReLU | (?, 128, 128, 16)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            max_pool | (?, 32, 128, 16)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Conv | (?, 32, 128, 64)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ReLU | (?, 32, 128, 64)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            max_pool | (?, 16, 64, 64)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Optimization -- Gradient Descent\n",
    "## 2.1 Cost function and Learning rate\n",
    "> ### Gradient Descent update the weight with: Learning_rate*slope\n",
    "> ![Gradient Descent](https://imgur.com/YQzIBuB.png)\n",
    ">> ### 1. Cost function:\n",
    "![Optimization](https://imgur.com/LXBjfLb.png)\n",
    ">> ### 2. Learning rate\n",
    "![Learning rate](https://imgur.com/J8U8fu9.jpg)\n",
    "\n",
    "> ## Meaning of cost function\n",
    ">> ### \"預測\"和\"實際\"誤差\n",
    "\n",
    "> ## Optimization Problems\n",
    ">> ### 1. 局部極小值\n",
    ">> ### 2. Saddle point\n",
    "\n",
    "> ## Overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "> ### 3. Check how the cost function looks like on the Internet, and explain how the weight changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(1)\n",
    "x = tf.placeholder(tf.float32,[None,1])\n",
    "\n",
    "### Try w=0,random_uniform for mode = 1,2,3 \n",
    "w = tf.Variable(0,dtype=tf.float32)\n",
    "#w = tf.Variable(tf.random_uniform([1], seed=2),dtype=tf.float32)\n",
    "###\n",
    "\n",
    "### change different cost function\n",
    "mode = 3\n",
    "###\n",
    "\n",
    "if mode == 1:\n",
    "    coefficients = np.array([[1.],[-10.],[25.]])\n",
    "    # operator overloading\n",
    "    # cost = tf.add(tf.add(w**2,tf.multiply(-10.,w),25))\n",
    "    cost = x[0]*w**2 + x[1]*w + x[2]\n",
    "elif mode == 2:\n",
    "    coefficients = np.array([[2.],[4.],[1.]])\n",
    "    # 0點沒有斜率\n",
    "    cost = x[0]*(w**4) + x[1]*(w**3) + x[2]*(w**2)\n",
    "elif mode == 3:\n",
    "    coefficients = np.array([[5.],[8.],[3.],[1.]])\n",
    "    cost = x[0]*(w**6)+ x[1]*(w**5)+ x[2]*(w**2)+ x[3]*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "lr=0.500000\tweight\t\tvalue update\n",
      "--------------------------------------------\n",
      "Initialize:\t0.000000\n",
      "Epoch 1:\t-0.500000\t-0.500000\n",
      "Epoch 2:\t-0.281250\t0.218750\n",
      "Epoch 3:\t-0.036244\t0.245006\n",
      "Epoch 4:\t-0.427545\t-0.391301\n",
      "Epoch 5:\t-0.098899\t0.328646\n",
      "Epoch 6:\t-0.303973\t-0.205074\n",
      "Epoch 7:\t-0.023879\t0.280094\n",
      "Epoch 8:\t-0.452248\t-0.428368\n",
      "Epoch 9:\t-0.148364\t0.303884\n",
      "Epoch 10:\t-0.211884\t-0.063520\n",
      "Epoch 11:\t-0.110137\t0.101747\n",
      "Epoch 12:\t-0.282425\t-0.172288\n",
      "Epoch 13:\t-0.035442\t0.246983\n",
      "Epoch 14:\t-0.429146\t-0.393704\n",
      "Epoch 15:\t-0.101721\t0.327425\n",
      "Epoch 16:\t-0.298536\t-0.196815\n",
      "Epoch 17:\t-0.026219\t0.272317\n",
      "Epoch 18:\t-0.447570\t-0.421351\n",
      "Epoch 19:\t-0.138015\t0.309555\n",
      "Epoch 20:\t-0.230476\t-0.092461\n",
      "Epoch 21:\t-0.085726\t0.144749\n",
      "Epoch 22:\t-0.329558\t-0.243832\n",
      "Epoch 23:\t-0.018489\t0.311069\n",
      "Epoch 24:\t-0.463024\t-0.444535\n",
      "Epoch 25:\t-0.173991\t0.289033\n",
      "Epoch 26:\t-0.167955\t0.006036\n",
      "Epoch 27:\t-0.178000\t-0.010044\n",
      "Epoch 28:\t-0.161398\t0.016602\n",
      "Epoch 29:\t-0.189133\t-0.027735\n",
      "Epoch 30:\t-0.143696\t0.045437\n",
      "Epoch 31:\t-0.220216\t-0.076520\n",
      "Epoch 32:\t-0.098835\t0.121381\n",
      "Epoch 33:\t-0.304097\t-0.205262\n",
      "Epoch 34:\t-0.023831\t0.280267\n",
      "Epoch 35:\t-0.452345\t-0.428515\n",
      "Epoch 36:\t-0.148585\t0.303760\n",
      "Epoch 37:\t-0.211492\t-0.062908\n",
      "Epoch 38:\t-0.110682\t0.100811\n",
      "Epoch 39:\t-0.281389\t-0.170707\n",
      "Epoch 40:\t-0.036149\t0.245240\n",
      "Epoch 41:\t-0.427736\t-0.391587\n",
      "Epoch 42:\t-0.099232\t0.328503\n",
      "Epoch 43:\t-0.303330\t-0.204098\n",
      "Epoch 44:\t-0.024135\t0.279195\n",
      "Epoch 45:\t-0.451737\t-0.427602\n",
      "Epoch 46:\t-0.147211\t0.304526\n",
      "Epoch 47:\t-0.213934\t-0.066723\n",
      "Epoch 48:\t-0.107304\t0.106630\n",
      "Epoch 49:\t-0.287830\t-0.180526\n",
      "Epoch 50:\t-0.031977\t0.255853\n",
      "Epoch 51:\t-0.436067\t-0.404090\n",
      "Epoch 52:\t-0.114525\t0.321541\n",
      "Epoch 53:\t-0.274094\t-0.159569\n",
      "Epoch 54:\t-0.041489\t0.232605\n",
      "Epoch 55:\t-0.417079\t-0.375589\n",
      "Epoch 56:\t-0.081734\t0.335345\n",
      "Epoch 57:\t-0.337371\t-0.255637\n",
      "Epoch 58:\t-0.018796\t0.318575\n",
      "Epoch 59:\t-0.462411\t-0.443616\n",
      "Epoch 60:\t-0.172466\t0.289945\n",
      "Epoch 61:\t-0.170473\t0.001993\n",
      "Epoch 62:\t-0.173785\t-0.003312\n",
      "Epoch 63:\t-0.168295\t0.005490\n",
      "Epoch 64:\t-0.177430\t-0.009135\n",
      "Epoch 65:\t-0.162324\t0.015105\n",
      "Epoch 66:\t-0.187546\t-0.025222\n",
      "Epoch 67:\t-0.146171\t0.041376\n",
      "Epoch 68:\t-0.215788\t-0.069617\n",
      "Epoch 69:\t-0.104771\t0.111017\n",
      "Epoch 70:\t-0.292679\t-0.187908\n",
      "Epoch 71:\t-0.029184\t0.263495\n",
      "Epoch 72:\t-0.441647\t-0.412463\n",
      "Epoch 73:\t-0.125572\t0.316074\n",
      "Epoch 74:\t-0.253360\t-0.127787\n",
      "Epoch 75:\t-0.060031\t0.193329\n",
      "Epoch 76:\t-0.380186\t-0.320155\n",
      "Epoch 77:\t-0.038328\t0.341858\n",
      "Epoch 78:\t-0.423385\t-0.385057\n",
      "Epoch 79:\t-0.091812\t0.331574\n",
      "Epoch 80:\t-0.317700\t-0.225888\n",
      "Epoch 81:\t-0.019802\t0.297898\n",
      "Epoch 82:\t-0.460399\t-0.440597\n",
      "Epoch 83:\t-0.167518\t0.292881\n",
      "Epoch 84:\t-0.178736\t-0.011218\n",
      "Epoch 85:\t-0.160204\t0.018532\n",
      "Epoch 86:\t-0.191183\t-0.030979\n",
      "Epoch 87:\t-0.140522\t0.050662\n",
      "Epoch 88:\t-0.225933\t-0.085412\n",
      "Epoch 89:\t-0.091416\t0.134517\n",
      "Epoch 90:\t-0.318468\t-0.227052\n",
      "Epoch 91:\t-0.019653\t0.298815\n",
      "Epoch 92:\t-0.460696\t-0.441042\n",
      "Epoch 93:\t-0.168242\t0.292454\n",
      "Epoch 94:\t-0.177517\t-0.009275\n",
      "Epoch 95:\t-0.162182\t0.015336\n",
      "Epoch 96:\t-0.187791\t-0.025609\n",
      "Epoch 97:\t-0.145788\t0.042002\n",
      "Epoch 98:\t-0.216470\t-0.070682\n",
      "Epoch 99:\t-0.103846\t0.112625\n",
      "Epoch 100:\t-0.294454\t-0.190608\n"
     ]
    }
   ],
   "source": [
    "### Coding 3: tune the learning rate\n",
    "# play for the value between: 0.9 ~ 0.01\n",
    "learning_rate = 0.5\n",
    "###\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "#first way\n",
    "'''\n",
    "session = tf.Session()\n",
    "session.run(init)\n",
    "print(session.run(w))\n",
    "'''\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"lr=%f\\tweight\\t\\tvalue update\" %learning_rate)\n",
    "print(\"--------------------------------------------\")\n",
    "#second way\n",
    "with tf.Session() as session:\n",
    "    # initialize\n",
    "    session.run(init)\n",
    "    print(\"Initialize:\\t%f\" %session.run(w))\n",
    "    wf = session.run(w)\n",
    "    for i in range(100):\n",
    "        # put input inside\n",
    "        session.run(train,feed_dict={x:coefficients})\n",
    "        # print the weight & 差異, think about the curve & the lowest point\n",
    "        print(\"Epoch %d:\\t%f\\t%f\" %(i+1,session.run(w),session.run(w)-wf))\n",
    "        wf = session.run(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
