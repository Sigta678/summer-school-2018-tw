{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function \n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics\n",
    "> ## 1. Build a model\n",
    ">> ## 1.1 Create placeholder\n",
    ">>> ## Coding 1: Dealing with tensors and dimensions\n",
    "\n",
    ">> ## 1.2 One-hot encoding\n",
    ">> ## 1.3 Define the weight of CNN\n",
    ">> ## 1.4 Understanding the self-defined blocks and distinguish the function of the layers\n",
    ">>> ## Coding 2: Add some layers in the model\n",
    "\n",
    "\n",
    "> ## 2. Optimization: Method of Gradient Descent\n",
    ">> ## 2.1 Cost function and Learning rate\n",
    ">>> ## Coding 3: Tune the learning rate on cost function\n",
    "\n",
    "\n",
    "> ## 3. Look into Pre-trained model: YOLOv3 (jupyter_2)\n",
    ">> ## Advance topic: ResNet\n",
    "\n",
    "> ## 4. Training small dataset, and plot the result on Tensorboard (jupyter_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a model\n",
    "## 1.1 Create placeholder: \n",
    "> ## Pre-allocate places for different inputs and output\n",
    "![Placeholder](https://imgur.com/lTf4ehx.png)\n",
    "\n",
    "> ## Discussion:\n",
    ">> ## 1. What's the meaning of <span style=\"color:blue\">None</span> in tf.placeholder( XX ,[<span style=\"color:blue\">None</span>, XX]) ?\n",
    ">> ## 2. What does it mean to create placeholder for input_picture and input_state ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_H0, n_W0, n_C0, n_y, n_state):\n",
    "    # Create float placeholder for input_picture\n",
    "    pic_placehold = tf.placeholder(tf.float32,[None,n_H0,n_W0,n_C0])\n",
    "    # Create integer placeholder for output_class\n",
    "    action_placehold = tf.placeholder(tf.int32,[None,n_y])\n",
    "    \n",
    "    ### Coding 1: Dealing with tensors and dimensions\n",
    "    ### Step 1: Create integer placeholder for input_state\n",
    "    ### START CODE HERE ### (1 line)\n",
    "    state_placehold = \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return pic_placehold, action_placehold, state_placehold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2: Tune and try out what the parameters in your tensor means\n",
    "pic_height = \n",
    "pic_width = \n",
    "pic_channel = \n",
    "output_class = \n",
    "input_state = \n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pic_placehold = Tensor(\"Placeholder:0\", shape=(?, 256, 256, 3), dtype=float32)\n",
      "action_placehold = Tensor(\"Placeholder_1:0\", shape=(?, 6), dtype=int32)\n",
      "state_placehold = Tensor(\"Placeholder_2:0\", shape=(?, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "pic_placehold, action_placehold, state_placehold = create_placeholders(pic_height, pic_width, pic_channel, output_class, input_state)\n",
    "print (\"pic_placehold = \" + str(pic_placehold))\n",
    "print (\"action_placehold = \" + str(action_placehold))\n",
    "print (\"state_placehold = \" + str(state_placehold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "\n",
    "<table> \n",
    "<tr>\n",
    "<td>\n",
    "    pic_placehold = Tensor(\"Placeholder:0\", shape=(?, 256, 256, 3), dtype=float32)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "    action_placehold  = Tensor(\"Placeholder_1:0\", shape=(?, 6), dtype=int32)\n",
    "\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<th>\n",
    "    state_placehold  = Tensor(\"Placeholder_2:0\", shape=(?, 4), dtype=int32)\n",
    "        \n",
    "</th>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 One-hot encoding\n",
    "> ## Expressions of probability on feature type\n",
    "> ## ![one-hot](https://imgur.com/jCjGQgi.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action = {0:\"right\", 1:\"left\", 2:\"front\", 3:\"back\", 4:\"up\", 5:\"down\"}\n",
    "action_type = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_onehot = tf.one_hot(action_placehold , action_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Define the weight of CNN: \n",
    "> ## Filters and channels: Place to keep parameters stored with its special meaning\n",
    "> ## Create parameters which can be stored and updated for back propagation\n",
    ">> ## Filter example\n",
    "![Sobel filter](https://imgur.com/YaAq4q7.png)\n",
    ">> ## After filter\n",
    "![Human](https://imgur.com/C8XqlHm.png)\n",
    ">> ## Channel\n",
    "![RGB](https://imgur.com/ygKlPRO.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(filter_height, filter_width, channel_in, channel_out):\n",
    "    \n",
    "    w_parameters = tf.Variable(tf.random_normal([filter_height,filter_width,channel_in,channel_out], stddev=0.1))\n",
    "    b_parameters = tf.Variable(tf.constant(0.1, shape=[channel_out]))\n",
    "    \n",
    "    return w_parameters, b_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your filter (at channel_in: 0 channel_out: 0) looks like this\n",
      "\n",
      "0.0230391\t-0.0697294\t-0.113742\t-0.028562\t-3.05606e-05\t\n",
      "\n",
      "0.118471\t-0.121192\t-0.0223764\t0.170336\t0.00057642\t\n",
      "\n",
      "-0.0319116\t-0.0790702\t0.0510799\t0.051996\t0.25189\t\n",
      "\n",
      "-0.0353221\t0.00756408\t0.0787909\t-0.119117\t0.00878545\t\n",
      "\n",
      "-0.140177\t-0.0323252\t0.0839279\t-0.0188945\t0.289834\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Tune the filter size if you want\n",
    "filter_height = 5\n",
    "filter_width = 5\n",
    "channel_in = 3\n",
    "channel_out = 8\n",
    "###\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess_test:\n",
    "    w,b = initialize_parameters(filter_height, filter_width, channel_in, channel_out)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess_test.run(init)\n",
    "    ### Watch for specific channel\n",
    "    c_in = 0 # 0 ~ channel_in-1\n",
    "    c_out = 0 # 0 ~ channel_out-1\n",
    "    ###\n",
    "    print(\"Your filter (at channel_in: %d channel_out: %d) looks like this\\n\" %(c_in,c_out) )\n",
    "    for i in range(0,int(w.shape[0])):\n",
    "        for j in range(0,int(w.shape[1])):\n",
    "            #print(w[i,j].shape)\n",
    "            # python3\n",
    "            print(w[i,j].eval()[c_in,c_out], end='\\t')\n",
    "            # python2\n",
    "            # print w[i,j].eval()[c_in,c_out],\n",
    "        print(\"\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Understanding the self-defined blocks and distinguish the function of the layers \n",
    "> ## CNN: strides, channel\n",
    "> ## CNN to NN: Flatten()\n",
    "> ## NN:  Fully_Connected()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Some definitions of functions \n",
    ">> ## Check on Tensorflow for some function used: https://www.tensorflow.org/api_docs/python/tf\n",
    "\n",
    "> ## Discussion:\n",
    ">> ## 1. What is stride? What is filter?\n",
    ">>> \n",
    "\n",
    ">> ## 2. How does \"Step 1\", \"Step 2\", \"Step 3\" happens?\n",
    ">>> ## CNN operation\n",
    ">>> ![CNN](https://imgur.com/FIy5Ou4.gif)\n",
    ">>> ## Max Pooling operation\n",
    ">>> ![Max Pool](https://imgur.com/ec0zNkC.png)\n",
    "\n",
    ">> ## 3. Problems in padding = 'SAME', and padding = 'VALID' ?\n",
    ">>> ## tf.nn.conv2d(XX, XX, XX, padding='SAME') \n",
    "\n",
    "> ## Hint for 1.4\n",
    "- ## <span style=\"color:red\">Graphic operations: CNN</span>\n",
    "- ## tf.Variable()\n",
    "- ## tf.nn.conv2d()\n",
    "- ## tf.nn.max_pool()\n",
    "- ## tf.contrib.layers.flatten()\n",
    "- ## tf.contrib.layers.fully_connected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create placeholder\n",
    "pic_height = 256\n",
    "pic_width = 256\n",
    "pic_channel = 3\n",
    "output_class = 6\n",
    "input_state = 4\n",
    "pic_placehold, action_placehold, state_placehold = create_placeholders(pic_height, pic_width, pic_channel, output_class, input_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the size of max pooling is pre-fixed in this self-defined function\n",
    "def conv_block(tensor_in, channel_in, channel_out, filter_height, filter_width, strides_height, strides_width, maxPool_height=2, maxPool_width=2):\n",
    "    tf.set_random_seed(1)\n",
    "    \n",
    "    w = tf.Variable(tf.truncated_normal([filter_height,filter_width,channel_in,channel_out], stddev=0.1))\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[channel_out]))\n",
    "    \n",
    "    conv = tf.nn.conv2d(tensor_in, w, strides=[1, strides_height, strides_width,1], padding='SAME')                \n",
    "    print(\"Conv\\t\\t%d\\t%d\\t%d\\t\\t| %s\" % (conv.shape[1],conv.shape[2],conv.shape[3],conv.shape))\n",
    "    # activate each nodes with bias\n",
    "    activate_func = tf.nn.relu(conv + b)\n",
    "    print(\"ReLU\\t\\t%d\\t%d\\t%d\\t\\t| %s\" % (activate_func.shape[1],activate_func.shape[2],activate_func.shape[3],activate_func.shape))\n",
    "    \n",
    "    maxPool = tf.nn.max_pool(activate_func, ksize=[1,maxPool_height,maxPool_width,1], strides=[1,maxPool_height,maxPool_width,1], padding='SAME')\n",
    "    print(\"max_pool\\t%d\\t%d\\t%d\\t\\t| %s\" %(maxPool.shape[1],maxPool.shape[2],maxPool.shape[3],maxPool.shape))\n",
    "    return maxPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "Layer\t\tHeight\tWidth\tChannel\t Node\t| What you will see\n",
      "---------------------------------------------------------------\n",
      "Input\t\t256\t256\t3\t\t| (?, 256, 256, 3)\n",
      "Conv\t\t128\t128\t8\t\t| (?, 128, 128, 8)\n",
      "ReLU\t\t128\t128\t8\t\t| (?, 128, 128, 8)\n",
      "max_pool\t64\t64\t8\t\t| (?, 64, 64, 8)\n"
     ]
    }
   ],
   "source": [
    "s = \"---------------------------------------------------------------\"\n",
    "print(s)\n",
    "print(\"Layer\\t\\tHeight\\tWidth\\tChannel\\t Node\\t| What you will see\")\n",
    "print(s)\n",
    "print(\"Input\\t\\t%d\\t%d\\t%d\\t\\t| %s\" %(pic_placehold.shape[1],pic_placehold.shape[2],pic_placehold.shape[3],pic_placehold.shape))\n",
    "\n",
    "### Coding 2: Add some layers in the model\n",
    "### Step 1: Tunning the stride for first CNN layer\n",
    "### Tunning part:\n",
    "strides_height = \n",
    "strides_width = \n",
    "filter_height = 4\n",
    "filter_width = 4\n",
    "###\n",
    "conv1 = conv_block(pic_placehold, 3, 8, filter_height, filter_width, strides_height, strides_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 1: Expected Output **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Input | (?, 256, 256, 3)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Conv | (?, 128, 128, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ReLU | (?, 128, 128, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            max_pool | (?, 64, 64, 8)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv\t\t16\t32\t16\t\t| (?, 16, 32, 16)\n",
      "ReLU\t\t16\t32\t16\t\t| (?, 16, 32, 16)\n",
      "max_pool\t8\t16\t16\t\t| (?, 8, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "### Step 2: Tunning the stride for second CNN layer and the channel\n",
    "### Tunning part: divisible stride\n",
    "strides_height =  \n",
    "strides_width = \n",
    "channel_in = \n",
    "channel_out = \n",
    "filter_height = 4\n",
    "filter_width = 4\n",
    "###\n",
    "conv2 = conv_block(conv1, channel_in, channel_out, filter_height, filter_width, strides_height, strides_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 2: Expected Output **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Conv | (?, 16, 32, 16)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ReLU | (?, 16, 32, 16)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            max_pool | (?, 8, 16, 16)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv\t\t4\t6\t32\t\t| (?, 4, 6, 32)\n",
      "ReLU\t\t4\t6\t32\t\t| (?, 4, 6, 32)\n",
      "max_pool\t2\t3\t32\t\t| (?, 2, 3, 32)\n"
     ]
    }
   ],
   "source": [
    "### Step 3: Tunning the stride and build the third block of CNN\n",
    "### Tunning part: non-divisible stride\n",
    "strides_height = \n",
    "strides_width = \n",
    "###\n",
    "conv3 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 3: Expected Output **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Conv | (?, 4, 6, 32)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ReLU | (?, 4, 6, 32)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            max_pool | (?, 2, 3, 32)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_block(tensor_in, node_out, activate_func):\n",
    "    node_in = int(tensor_in.shape[1])\n",
    "    w = tf.Variable(tf.random_normal([node_in, node_out], stddev=0.35))\n",
    "    b = tf.Variable(tf.zeros([node_out]))\n",
    "    \n",
    "    if activate_func == None:\n",
    "        ### 2 way\n",
    "        #fully_connect = tf.contrib.layers.fully_connected(tensor_in, num_outputs= node_out,activation_fn= None)\n",
    "        fully_connect = tf.add(tf.matmul(tensor_in,w), b)\n",
    "        ###\n",
    "        print(\"fully_connect\\t\\t\\t\\t%d\\t| %s\" % (fully_connect.shape[1],fully_connect.shape))\n",
    "    elif activate_func == tf.nn.relu:\n",
    "        ### 2 way\n",
    "        #fully_connect = tf.contrib.layers.fully_connected(tensor_in, num_outputs= node_out,activation_fn= activate_func)\n",
    "        fully_connect = tf.nn.relu(tf.add(tf.matmul(tensor_in,w), b))\n",
    "        ###\n",
    "        print(\"fully_connect\\t\\t\\t\\t%d\\t| %s\" % (fully_connect.shape[1],fully_connect.shape))\n",
    "        print(\"ReLU\\t\\t\\t\\t\\t%d\\t| %s\" % (fully_connect.shape[1],fully_connect.shape))\n",
    "    elif activate_func == tf.nn.softmax:\n",
    "        ### 2 way\n",
    "        #fully_connect = tf.contrib.layers.fully_connected(tensor_in, num_outputs= node_out,activation_fn= activate_func)\n",
    "        fully_connect = tf.nn.softmax(tf.add(tf.matmul(tensor_in,w), b))\n",
    "        ###\n",
    "        print(\"fully_connect\\t\\t\\t\\t%d\\t| %s\" % (fully_connect.shape[1],fully_connect.shape))\n",
    "        print(\"Softmax\\t\\t\\t\\t\\t%d\\t| %s\" % (fully_connect.shape[1],fully_connect.shape))\n",
    "    \n",
    "    return fully_connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "Layer\t\tHeight\tWidth\tChannel\t Node\t| What you will see\n",
      "---------------------------------------------------------------\n",
      "Input\t\t256\t256\t3\t\t| (?, 256, 256, 3)\n",
      "Conv\t\t128\t128\t16\t\t| (?, 128, 128, 16)\n",
      "ReLU\t\t128\t128\t16\t\t| (?, 128, 128, 16)\n",
      "max_pool\t32\t128\t16\t\t| (?, 32, 128, 16)\n",
      "Conv\t\t32\t128\t64\t\t| (?, 32, 128, 64)\n",
      "ReLU\t\t32\t128\t64\t\t| (?, 32, 128, 64)\n",
      "max_pool\t16\t64\t64\t\t| (?, 16, 64, 64)\n",
      "flatten\t\t\t\t\t65536\t| (?, 65536)\n",
      "fully_connect\t\t\t\t256\t| (?, 256)\n",
      "fully_connect\t\t\t\t128\t| (?, 128)\n",
      "ReLU\t\t\t\t\t128\t| (?, 128)\n",
      "fully_connect\t\t\t\t6\t| (?, 6)\n",
      "Softmax\t\t\t\t\t6\t| (?, 6)\n"
     ]
    }
   ],
   "source": [
    "# Summarized with CNN and NN\n",
    "s = \"---------------------------------------------------------------\"\n",
    "print(s)\n",
    "print(\"Layer\\t\\tHeight\\tWidth\\tChannel\\t Node\\t| What you will see\")\n",
    "print(s)\n",
    "print(\"Input\\t\\t%d\\t%d\\t%d\\t\\t| %s\" %(pic_placehold.shape[1],pic_placehold.shape[2],pic_placehold.shape[3],pic_placehold.shape))\n",
    "\n",
    "### Step 4: Select max_pooling size, and build your layers from the beginning\n",
    "conv1 = \n",
    "conv2 = \n",
    "###\n",
    "\n",
    "#flat = tf.contrib.layers.flatten(conv2)\n",
    "flat = tf.reshape(conv2, [-1, int(conv2.shape[1])*int(conv2.shape[2])*int(conv2.shape[3])])\n",
    "print(\"flatten\\t\\t\\t\\t\\t%d\\t| %s\" %(flat.shape[1],flat.shape))\n",
    "\n",
    "fc_layer1 = fc_block(flat, 256, None)\n",
    "fc_layer2 = fc_block(fc_layer1, 128, tf.nn.relu)\n",
    "predicted_prob = fc_block(fc_layer1, output_class, tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 4: Expected Output **\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Input | (?, 256, 256, 3)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Conv | (?, 128, 128, 16)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ReLU | (?, 128, 128, 16)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            max_pool | (?, 32, 128, 16)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            Conv | (?, 32, 128, 64)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            ReLU | (?, 32, 128, 64)\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            max_pool | (?, 16, 64, 64)\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Optimization: Method of gradient descent\n",
    "## 2.1 Cost function and learning rate\n",
    "> ## Method of gradient descent updates the weight with: learning_rate*slope\n",
    "> ![Gradient Descent](https://imgur.com/YQzIBuB.png)\n",
    ">> ## 1. Cost function:\n",
    "![Optimization](https://imgur.com/LXBjfLb.png)\n",
    ">> ## 2. Learning rate\n",
    "![Learning rate](https://imgur.com/J8U8fu9.jpg)\n",
    "\n",
    "> ## Meaning of cost function\n",
    ">> ## Difference between \"predict\" and \"reality\"\n",
    "\n",
    "> ## Optimization problems: Find global minimum\n",
    ">> ## 1. Local minimum\n",
    ">> ## 2. Saddle point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion:\n",
    "> ## 1. Check how the cost function looks like on the Internet, and explain how the weight changes.\n",
    ">> ## Link for mode = 2: [Wolfram Alpha](http://www.wolframalpha.com/input/?i=2*w%5E4+%2B+4*w%5E3+%2B+1*w%5E2)\n",
    "\n",
    "> ## 2. What is \"overfitting\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(1)\n",
    "x = tf.placeholder(tf.float32,[None,1])\n",
    "\n",
    "### Try w=0,random_uniform for mode = 1,2,3 \n",
    "w = tf.Variable(0,dtype=tf.float32)\n",
    "#w = tf.Variable(tf.random_uniform([1], seed=2),dtype=tf.float32)\n",
    "###\n",
    "\n",
    "### change different cost function\n",
    "mode = 1\n",
    "###\n",
    "\n",
    "if mode == 1:\n",
    "    coefficients = np.array([[1.],[-10.],[25.]])\n",
    "    # operator overloading\n",
    "    # cost = tf.add(tf.add(w**2,tf.multiply(-10.,w),25))\n",
    "    cost = x[0]*w**2 + x[1]*w + x[2]\n",
    "elif mode == 2:\n",
    "    coefficients = np.array([[2.],[4.],[1.]])\n",
    "    cost = x[0]*(w**4) + x[1]*(w**3) + x[2]*(w**2)\n",
    "elif mode == 3:\n",
    "    coefficients = np.array([[5.],[8.],[3.],[1.]])\n",
    "    cost = x[0]*(w**6)+ x[1]*(w**5)+ x[2]*(w**2)+ x[3]*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------\n",
      "lr=0.010000\tweight\t\tvalue update\n",
      "--------------------------------------------\n",
      "Initialize:\t0.000000\n",
      "Epoch 1:\t0.100000\t0.100000\n",
      "Epoch 2:\t0.198000\t0.098000\n",
      "Epoch 3:\t0.294040\t0.096040\n",
      "Epoch 4:\t0.388159\t0.094119\n",
      "Epoch 5:\t0.480396\t0.092237\n",
      "Epoch 6:\t0.570788\t0.090392\n",
      "Epoch 7:\t0.659372\t0.088584\n",
      "Epoch 8:\t0.746185\t0.086813\n",
      "Epoch 9:\t0.831261\t0.085076\n",
      "Epoch 10:\t0.914636\t0.083375\n",
      "Epoch 11:\t0.996343\t0.081707\n",
      "Epoch 12:\t1.076416\t0.080073\n",
      "Epoch 13:\t1.154888\t0.078472\n",
      "Epoch 14:\t1.231790\t0.076902\n",
      "Epoch 15:\t1.307155\t0.075364\n",
      "Epoch 16:\t1.381011\t0.073857\n",
      "Epoch 17:\t1.453391\t0.072380\n",
      "Epoch 18:\t1.524323\t0.070932\n",
      "Epoch 19:\t1.593837\t0.069514\n",
      "Epoch 20:\t1.661960\t0.068123\n",
      "Epoch 21:\t1.728721\t0.066761\n",
      "Epoch 22:\t1.794147\t0.065426\n",
      "Epoch 23:\t1.858264\t0.064117\n",
      "Epoch 24:\t1.921098\t0.062835\n",
      "Epoch 25:\t1.982676\t0.061578\n",
      "Epoch 26:\t2.043023\t0.060346\n",
      "Epoch 27:\t2.102162\t0.059139\n",
      "Epoch 28:\t2.160119\t0.057957\n",
      "Epoch 29:\t2.216917\t0.056798\n",
      "Epoch 30:\t2.272578\t0.055662\n",
      "Epoch 31:\t2.327127\t0.054549\n",
      "Epoch 32:\t2.380584\t0.053457\n",
      "Epoch 33:\t2.432973\t0.052388\n",
      "Epoch 34:\t2.484313\t0.051341\n",
      "Epoch 35:\t2.534627\t0.050314\n",
      "Epoch 36:\t2.583934\t0.049307\n",
      "Epoch 37:\t2.632256\t0.048321\n",
      "Epoch 38:\t2.679610\t0.047355\n",
      "Epoch 39:\t2.726018\t0.046408\n",
      "Epoch 40:\t2.771498\t0.045480\n",
      "Epoch 41:\t2.816068\t0.044570\n",
      "Epoch 42:\t2.859746\t0.043679\n",
      "Epoch 43:\t2.902551\t0.042805\n",
      "Epoch 44:\t2.944500\t0.041949\n",
      "Epoch 45:\t2.985610\t0.041110\n",
      "Epoch 46:\t3.025898\t0.040288\n",
      "Epoch 47:\t3.065380\t0.039482\n",
      "Epoch 48:\t3.104073\t0.038692\n",
      "Epoch 49:\t3.141991\t0.037919\n",
      "Epoch 50:\t3.179152\t0.037160\n",
      "Epoch 51:\t3.215569\t0.036417\n",
      "Epoch 52:\t3.251257\t0.035689\n",
      "Epoch 53:\t3.286232\t0.034975\n",
      "Epoch 54:\t3.320507\t0.034275\n",
      "Epoch 55:\t3.354097\t0.033590\n",
      "Epoch 56:\t3.387015\t0.032918\n",
      "Epoch 57:\t3.419275\t0.032260\n",
      "Epoch 58:\t3.450889\t0.031615\n",
      "Epoch 59:\t3.481872\t0.030982\n",
      "Epoch 60:\t3.512234\t0.030363\n",
      "Epoch 61:\t3.541990\t0.029755\n",
      "Epoch 62:\t3.571150\t0.029160\n",
      "Epoch 63:\t3.599727\t0.028577\n",
      "Epoch 64:\t3.627732\t0.028005\n",
      "Epoch 65:\t3.655178\t0.027445\n",
      "Epoch 66:\t3.682074\t0.026896\n",
      "Epoch 67:\t3.708433\t0.026359\n",
      "Epoch 68:\t3.734264\t0.025831\n",
      "Epoch 69:\t3.759579\t0.025315\n",
      "Epoch 70:\t3.784387\t0.024808\n",
      "Epoch 71:\t3.808700\t0.024312\n",
      "Epoch 72:\t3.832526\t0.023826\n",
      "Epoch 73:\t3.855875\t0.023350\n",
      "Epoch 74:\t3.878758\t0.022882\n",
      "Epoch 75:\t3.901183\t0.022425\n",
      "Epoch 76:\t3.923159\t0.021976\n",
      "Epoch 77:\t3.944696\t0.021537\n",
      "Epoch 78:\t3.965802\t0.021106\n",
      "Epoch 79:\t3.986486\t0.020684\n",
      "Epoch 80:\t4.006756\t0.020270\n",
      "Epoch 81:\t4.026621\t0.019865\n",
      "Epoch 82:\t4.046088\t0.019467\n",
      "Epoch 83:\t4.065166\t0.019078\n",
      "Epoch 84:\t4.083863\t0.018697\n",
      "Epoch 85:\t4.102186\t0.018323\n",
      "Epoch 86:\t4.120142\t0.017956\n",
      "Epoch 87:\t4.137740\t0.017597\n",
      "Epoch 88:\t4.154985\t0.017245\n",
      "Epoch 89:\t4.171885\t0.016901\n",
      "Epoch 90:\t4.188448\t0.016562\n",
      "Epoch 91:\t4.204679\t0.016231\n",
      "Epoch 92:\t4.220585\t0.015906\n",
      "Epoch 93:\t4.236174\t0.015588\n",
      "Epoch 94:\t4.251450\t0.015276\n",
      "Epoch 95:\t4.266421\t0.014971\n",
      "Epoch 96:\t4.281093\t0.014672\n",
      "Epoch 97:\t4.295471\t0.014378\n",
      "Epoch 98:\t4.309561\t0.014091\n",
      "Epoch 99:\t4.323370\t0.013809\n",
      "Epoch 100:\t4.336903\t0.013533\n"
     ]
    }
   ],
   "source": [
    "### Coding 3: tune the learning rate\n",
    "### play for the value between: 0.9 ~ 0.01\n",
    "learning_rate = 0.01\n",
    "###\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "print(\"--------------------------------------------\")\n",
    "print(\"lr=%f\\tweight\\t\\tvalue update\" %learning_rate)\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    print(\"Initialize:\\t%f\" %session.run(w))\n",
    "    wf = session.run(w)\n",
    "    for i in range(100):\n",
    "        # feed_dict={x:coefficients}, feed our data \"coefficients\" into our placeholder \"x\"\n",
    "        session.run(train,feed_dict={x:coefficients})\n",
    "        print(\"Epoch %d:\\t%f\\t%f\" %(i+1,session.run(w),session.run(w)-wf))\n",
    "        wf = session.run(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
