{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduce to OpenAI Gym\n",
    "\n",
    "We will introduce the main API methods that users of this class need to know are:\n",
    "* reset\n",
    "* step\n",
    "* render\n",
    "\n",
    "### DuckieNav-v0 Environment\n",
    "The example is modifed from the Taxi Problem in \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\" by  Tom Dietterich (2000), Journal of Artificial Intelligence Research.\n",
    "\n",
    "<img style=\"float: right;\" src=\"images/DuckieNav-v1.png\"  width=\"240\" height=\"240\">\n",
    "\n",
    "\n",
    "```\n",
    "MAP = [\n",
    "    \"+-----------------+\",\n",
    "    \"|O|O| : : : : :G: |\",\n",
    "    \"|O|O| |O| |O| |O| |\",\n",
    "    \"|O| : |O| |O| |O| |\",\n",
    "    \"| : |O|O| : : : : |\",\n",
    "    \"| |O|O|O|O|O| |O| |\",\n",
    "    \"| : :R: : : : :O: |\",\n",
    "    \"| |O|O|O| |O|O|O| |\",\n",
    "    \"| |O| : : |O| : : |\",\n",
    "    \"| |O| |O|O|O|B|O| |\",\n",
    "    \"| : : : : : : |O| |\",\n",
    "    \"| |O| |O| |O| |O| |\",\n",
    "    \"| : : : : : : |O| |\",\n",
    "    \"| |O| |O| |O|O|O| |\",\n",
    "    \"| : : :Y: : : : : |\",\n",
    "    \"+-----------------+\",\n",
    "]\n",
    "```\n",
    "\n",
    "We consider shows a 14 by 9 grid world inhabited by a Duckietown, except the \"service area.\" The taxi problem is episodic, and in each episode a passenger is located at one of the 4 specially designated locations (R, Y, B, and G). The Duckiebot (taxi agent) starts in a given location and must go to the transported passengerâ€™s location, pick up the passenger, go to the destination location, and put down the passenger. The episode ends when the passenger is deposited at the destination location to one of the 4 locations.\n",
    "\n",
    "Adapted from https://www.oreilly.com/learning/introduction-to-reinforcement-learning-and-openai-gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize DuckieNav-v0\n",
    "\n",
    "### Installation\n",
    "You can obtain and install this customized gym environment: \n",
    "```\n",
    "$ git clone https://github.com/ARG-NCTU/gym-duckienav.git\n",
    "$ cd gym-duckienav\n",
    "$ pip install -e . # you may need sudo depending on your setup\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_duckienav\n",
    "import gym.spaces\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"DuckieNav-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many 'states' in observation_space: \n",
    "There are 2520 states from: 14 (rows) x 9 (columns) x 5 (passenger locations: R, Y, B, G, or on taxi) x 4 (destinations: R, Y, B, or G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2520"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### action_space: \n",
    "There are 6 possible actions in Taxi-v2 environment\n",
    "* down (0), up (1), right (2), left (3), pick-up (4), and drop-off (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. States\n",
    "\n",
    "Resets the state of the environment and returns an initial observation (state).\n",
    "\n",
    "The current state is from :\n",
    "* current taxi row position\n",
    "* current taxi colum position\n",
    "* passenger location (Blue or in taxi) from 0: R, 1: G, 2: Y, 3: B; 4: in taxi.\n",
    "* destination location (Magenta) from 0: R, 1: G, 2: Y, 3: B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: 1563\n",
      "8\n",
      "6\n",
      "0\n",
      "3\n",
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[34;1mR\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|\u001b[35m\u001b[43mB\u001b[0m\u001b[0m|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "print \"Current state: \" + str(env.s)\n",
    "for p in env.decode(env.s): print p\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat previous cell for a few times.\n",
    "\n",
    "In taxi problem, the colors mean:\n",
    "* blue: passenger's current position\n",
    "* magenta: destination\n",
    "* yellow: empty taxi\n",
    "* green: full taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Actions\n",
    "\n",
    "Remember that the taxi agent can perform the following actions:\n",
    "* 0: \"South\", \n",
    "* 1: \"North\", \n",
    "* 2: \"East\", \n",
    "* 3: \"West\", \n",
    "* 4: \"Pickup\", \n",
    "* 5: \"Dropoff\"\n",
    "\n",
    "Let's set the state to 124.\n",
    "Let the taxi agent perform some actions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : :\u001b[43m \u001b[0m:\u001b[34;1mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35mR\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "env.s = 124\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `step()`\n",
    "\n",
    "Run one timestep of the environment's dynamics. \n",
    "It returns a tuple (observation, reward, done, info)\n",
    "* observation (object): agent's observation of the current environment\n",
    "* reward (float) : amount of reward returned after previous action\n",
    "* done (boolean): whether the episode has ended, in which case further step() calls will return undefined results\n",
    "* info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
    "\n",
    "Essentially the empty taxi is supposed to: \n",
    "* move toward the blue letter, \n",
    "* pickup the passenger (now the taxi is green), \n",
    "* drive to the magenta letter, and \n",
    "* drop the passenger (the taxi is yellow again).\n",
    "\n",
    "It is obvious that we should start with moving \"East\" env.step(2). Index 2 is for moving \"East\"\n",
    "We will do the followings:\n",
    "* Perform \"Pickup\" step(4) (although the passenger is not here)\n",
    "* Perform \"East\" step(2)\n",
    "* Perform \"Pickup\" step(4)\n",
    "* Perform \"West\" step(3)\n",
    "* Perform \"South\" step(0) for 5 times\n",
    "* Perfomr \"Dropoff\" (5)\n",
    "* Perform \"West\" step(3) for 4 times\n",
    "* Perfomr \"Dropoff\" (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : :\u001b[43m \u001b[0m:\u001b[34;1mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35mR\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (Pickup)\n",
      "reward: -10\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(4)\n",
    "env.render()\n",
    "print \"reward: \" + str(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35mR\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (East)\n",
      "reward: -1\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(2)\n",
    "env.render()\n",
    "print \"reward: \" + str(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :\u001b[42mG\u001b[0m: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35mR\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (Pickup)\n",
      "reward: -1\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(4)\n",
    "env.render()\n",
    "print \"reward: \" + str(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : :\u001b[42m_\u001b[0m:G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35mR\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (West)\n",
      "reward: -1\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(3)\n",
    "env.render()\n",
    "print \"reward: \" + str(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35mR\u001b[0m: : : :\u001b[42m_\u001b[0m:O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (South)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    env.step(0)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35mR\u001b[0m: : : :\u001b[42m_\u001b[0m:O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (Dropoff)\n",
      "reward: -10\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(5)\n",
    "env.render()\n",
    "print \"reward: \" + str(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (West)\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 4):\n",
    "    env.step(3)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|O|O| : : : : :G: |\n",
      "|O|O| |O| |O| |O| |\n",
      "|O| : |O| |O| |O| |\n",
      "| : |O|O| : : : : |\n",
      "| |O|O|O|O|O| |O| |\n",
      "| : :\u001b[35m\u001b[42mR\u001b[0m\u001b[0m: : : : :O: |\n",
      "| |O|O|O| |O|O|O| |\n",
      "| |O| : : |O| : : |\n",
      "| |O| |O|O|O|B|O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O| |O| |\n",
      "| : : : : : : |O| |\n",
      "| |O| |O| |O|O|O| |\n",
      "| : : :Y: : : : : |\n",
      "+-----------------+\n",
      "  (Dropoff)\n",
      "reward: 20\n"
     ]
    }
   ],
   "source": [
    "state, reward, done, info = env.step(5)\n",
    "env.render()\n",
    "print \"reward: \" + str(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards\n",
    "\n",
    "You have probably figured out the rewards:\n",
    "* Perform any movements: -1\n",
    "* Pick up or drop off at the wrong position: -10\n",
    "* Drop off the passenger at the right position: 20 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ramdon Actions\n",
    "\n",
    "We will use the funciton env.action_space.sample(); you could run the following cell for a few times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How good does behaving completely random do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved in 7410 Steps with a total reward of -30321\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "counter = 0\n",
    "g = 0\n",
    "reward = None\n",
    "while reward != 20:\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    counter += 1\n",
    "    g += reward\n",
    "print(\"Solved in {} Steps with a total reward of {}\".format(counter,g))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Basic Reinforcement Learning: Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State = 1673\n",
      "Final State = 157\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "Q = np.zeros([n_states, n_actions])\n",
    "\n",
    "# This multidimensional array will keep a history of our Q-Values for all states\n",
    "Q_hist = np.zeros([n_states, n_actions, 0])\n",
    "\n",
    "\n",
    "episodes = 1\n",
    "G = 0\n",
    "alpha = 0.618\n",
    "\n",
    "for episode in range(1,episodes+1):\n",
    "    done = False\n",
    "    G, reward = 0,0\n",
    "    state = env.reset()\n",
    "    firstState = state\n",
    "    print(\"Initial State = {}\".format(state))\n",
    "    while reward != 20:\n",
    "        action = np.argmax(Q[state]) \n",
    "        state2, reward, done, info = env.step(action)\n",
    "        Q[state,action] += alpha * (reward + np.max(Q[state2]) - Q[state,action]) \n",
    "        G += reward\n",
    "        state = state2\n",
    "        \n",
    "        #This will keep a history of Q Values in a multi dimensional array\n",
    "        Q_hist = np.dstack((Q_hist, Q))\n",
    "finalState = state\n",
    "print(\"Final State = {}\".format(finalState))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
